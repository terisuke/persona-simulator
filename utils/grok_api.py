"""
Grok APIé€£æºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
XæŠ•ç¨¿ã®å–å¾—ã¨LLMç”Ÿæˆã‚’æ‹…å½“
"""

import requests
import os
import logging
import json
from typing import List, Dict, Optional
from datetime import datetime
from .error_handler import (
    ErrorHandler, 
    PerformanceLogger,
    APIConnectionError,
    log_function_call
)

logger = logging.getLogger(__name__)


def log_structured_api_call(
    source: str,
    account: str = None,
    rate_limit_remaining: Optional[int] = None,
    reset_at: Optional[str] = None,
    generated_flag: bool = False,
    **kwargs
):
    """
    æ§‹é€ åŒ–ãƒ­ã‚°ã‚’å‡ºåŠ›ï¼ˆgrok_api.pyç”¨ï¼‰
    
    Args:
        source: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆtwitter/web_search/generatedï¼‰
        account: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå
        rate_limit_remaining: ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæ®‹ã‚Šå›æ•°
        reset_at: ãƒªã‚»ãƒƒãƒˆæ™‚åˆ»ï¼ˆISOå½¢å¼æ–‡å­—åˆ—ï¼‰
        generated_flag: ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ©ã‚°
        **kwargs: ãã®ä»–ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    """
    log_data = {
        "source": source,
        "generated_flag": generated_flag,
    }
    
    if account:
        log_data["account"] = account
    
    if rate_limit_remaining is not None:
        log_data["rate_limit_remaining"] = rate_limit_remaining
    
    if reset_at:
        log_data["reset_at"] = reset_at
    
    if kwargs:
        log_data.update(kwargs)
    
    logger.info(f"[STRUCTURED] {json.dumps(log_data, ensure_ascii=False)}")

# å®šæ•°å®šç¾©
MAX_CITATION_POSTS = 3  # å¼•ç”¨ã™ã‚‹æŠ•ç¨¿ã®æœ€å¤§æ•°
# ç”Ÿæˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨±å¯å¯å¦ï¼ˆé‹ç”¨ã§ã¯ False ã‚’å¼·åˆ¶ï¼‰
ALLOW_GENERATED_DEFAULT = False

# ãƒ—ãƒªã‚»ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé »å‡ºåˆ†é‡ï¼‰
PRESET_KEYWORDS = {
    "ai_engineer": "AI engineer",
    "data_scientist": "data scientist",
    "ml_engineer": "machine learning engineer",
    "startup_founder": "startup founder",
    "tech_executive": "tech executive",
    "venture_capital": "venture capitalist",
    "cybersecurity": "cybersecurity expert",
    "cloud_architect": "cloud architect",
    "devops_engineer": "DevOps engineer",
    "blockchain_developer": "blockchain developer",
    "product_manager": "product manager",
    "ux_designer": "UX designer",
    "software_engineer": "software engineer",
    "open_source": "open source contributor",
    "tech_writer": "tech writer",
    "data_engineer": "data engineer"
}

# å“è³ªåŸºæº–ï¼ˆã‚¢ã‚«ã‚¦ãƒ³ãƒˆç™ºè¦‹æ™‚ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ä½¿ç”¨ï¼‰
# å®Ÿä¸–ç•ŒæŒ‡æ¨™ï¼ˆX APIãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰ãƒ™ãƒ¼ã‚¹ã§è©•ä¾¡
QUALITY_THRESHOLDS = {
    'min_followers': 100,          # æœ€å°ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°
    'min_tweet_count': 50,         # æœ€å°ãƒ„ã‚¤ãƒ¼ãƒˆæ•°ï¼ˆæŠ•ç¨¿æ•°ï¼‰
    'max_days_inactive': 180,      # æœ€å¤§éã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ—¥æ•°ï¼ˆæœ€çµ‚ãƒ„ã‚¤ãƒ¼ãƒˆã‹ã‚‰ï¼‰
    'min_quality_score': 0.6      # æœ€å°å“è³ªã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ï¼‰
}


class GrokAPI:
    """Grok APIã¨ã®é€£æºã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    BASE_URL = "https://api.x.ai/v1"
    
    # åˆ©ç”¨å¯èƒ½ãªGrokãƒ¢ãƒ‡ãƒ«ï¼ˆ2025å¹´10æœˆæ™‚ç‚¹ï¼‰
    # - grok-4-fast-reasoning: æœ€æ–°ãƒ»é«˜é€Ÿæ¨è«–ãƒ¢ãƒ‡ãƒ«ï¼ˆæ¨å¥¨ï¼‰
    # - grok-3: æ¨™æº–ãƒ¢ãƒ‡ãƒ«
    # - grok-beta: å»ƒæ­¢æ¸ˆã¿ï¼ˆ2025å¹´9æœˆ15æ—¥ï¼‰
    DEFAULT_MODEL = "grok-4-fast-reasoning"
    
    def __init__(self, api_key: str, model: str = None):
        """
        Args:
            api_key: Grok APIã‚­ãƒ¼
            model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: grok-4-fast-reasoningï¼‰
        """
        self.api_key = api_key
        self.model = model or self.DEFAULT_MODEL
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        self.conversation_history = []  # ä¼šè©±å±¥æ­´
        self.last_response_id = None    # æœ€å¾Œã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ID
    
    @log_function_call
    def fetch_posts(
        self, 
        account: str, 
        limit: int = 20, 
        since_date: str = "2024-01-01",
        x_api_client=None,
        max_rate_wait_seconds: int = 900,
        allow_generated: Optional[bool] = None
    ) -> List[Dict]:
        """
        æŒ‡å®šã•ã‚ŒãŸXã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æŠ•ç¨¿ã‚’å–å¾—
        
        å–å¾—å„ªå…ˆé †ä½:
        1. X API v2 (fetch_user_tweets)
        2. X API v2 (search_recent_tweets with from:username)
        3. Grok Realtime Web Search
        4. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ³ãƒ—ãƒ«æŠ•ç¨¿ç”Ÿæˆ
        
        Args:
            account: Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆåï¼ˆ@ä»˜ãã§ã‚‚å¯ï¼‰
            limit: å–å¾—ã™ã‚‹æŠ•ç¨¿æ•°
            since_date: ã“ã®æ—¥ä»˜ä»¥é™ã®æŠ•ç¨¿ã‚’å–å¾—ï¼ˆX APIä½¿ç”¨æ™‚ï¼‰
            x_api_client: X APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            max_rate_wait_seconds: X APIåˆ©ç”¨æ™‚ã«å¾…æ©Ÿã™ã‚‹æœ€å¤§ç§’æ•°ï¼ˆUIã§ã¯0ãªã©çŸ­ã‚ã«è¨­å®šï¼‰
            
        Returns:
            æŠ•ç¨¿ãƒªã‚¹ãƒˆ [{"id": str, "text": str, "link": str, "date": str}]
        """
        # @ã‚’å‰Šé™¤
        account = account.lstrip("@")
        
        # X API v2ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯å®ŸæŠ•ç¨¿ã‚’å–å¾—
        if x_api_client:
            # æ–¹æ³•1: ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãƒ™ãƒ¼ã‚¹ã®å–å¾—ã‚’è©¦è¡Œ
            try:
                logger.info(f"[æ–¹æ³•1] X APIã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å–å¾—ä¸­: @{account}")
                posts = x_api_client.fetch_user_tweets(
                    account,
                    max_results=limit,
                    max_wait_seconds=max_rate_wait_seconds
                )
                if posts:
                    logger.info(f"âœ… X API (fetch_user_tweets) æˆåŠŸ: {len(posts)}ä»¶")
                    # æ§‹é€ åŒ–ãƒ­ã‚°ã¯ x_api_client å†…ã§å‡ºåŠ›æ¸ˆã¿
                    return posts
            except Exception as e:
                logger.warning(f"[æ–¹æ³•1] å¤±æ•—: {str(e)}")
            
            # æ–¹æ³•2: æ¤œç´¢APIã‚’ä½¿ç”¨ï¼ˆfrom:username ã‚¯ã‚¨ãƒªï¼‰
            try:
                logger.info(f"[æ–¹æ³•2] X APIæ¤œç´¢ã‚’è©¦è¡Œä¸­: from:{account}")
                search_query = f"from:{account} -is:retweet -is:reply"
                posts = x_api_client.search_recent_tweets(
                    search_query,
                    max_results=limit,
                    max_wait_seconds=max_rate_wait_seconds
                )
                if posts:
                    logger.info(f"âœ… X API (search_recent_tweets) æˆåŠŸ: {len(posts)}ä»¶")
                    # æ§‹é€ åŒ–ãƒ­ã‚°ã¯ x_api_client å†…ã§å‡ºåŠ›æ¸ˆã¿
                    return posts
            except Exception as e:
                logger.warning(f"[æ–¹æ³•2] å¤±æ•—: {str(e)}")
            
            logger.info("X APIä¸¡æ–¹å¤±æ•—ã€æ¬¡ã®æ–¹æ³•ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        
        # æ–¹æ³•3: Grok Realtime Web Searchã§å®ŸæŠ•ç¨¿ã‚’å–å¾—
        logger.info(f"[æ–¹æ³•3] Grok Web Searchã§å®ŸæŠ•ç¨¿ã‚’æ¤œç´¢ä¸­: @{account}")
        # ç’°å¢ƒã«å¿œã˜ãŸæ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä¾‹: è¨€èªãƒ»åœ°åŸŸï¼‰ã‚’ä»˜ä¸
        search_params = {
            "lang": os.environ.get("GROK_SEARCH_LANG"),
            "region": os.environ.get("GROK_SEARCH_REGION")
        }
        web_posts = self._fetch_posts_via_web_search(account, limit, search_parameters=search_params)
        if web_posts:
            logger.info(f"âœ… Grok Web Search æˆåŠŸ: {len(web_posts)}ä»¶")
            log_structured_api_call(
                source="web_search",
                account=account,
                generated_flag=False,
                post_count=len(web_posts)
            )
            return web_posts
        
        # æ–¹æ³•4: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ - LLMã§ã‚µãƒ³ãƒ—ãƒ«æŠ•ç¨¿ç”Ÿæˆ
        logger.info(f"[æ–¹æ³•4] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ³ãƒ—ãƒ«æŠ•ç¨¿ã‚’ç”Ÿæˆä¸­: @{account} (limit={limit})")
        # é‹ç”¨ãƒãƒªã‚·ãƒ¼: æ˜ç¤ºçš„ã«è¨±å¯ã•ã‚Œãªã„é™ã‚Šã€ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã•ãªã„
        allow = ALLOW_GENERATED_DEFAULT if allow_generated is None else bool(allow_generated)
        if not allow:
            logger.warning("ç”Ÿæˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ï¼ˆallow_generated=Falseï¼‰ã€‚ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™ã€‚")
            log_structured_api_call(
                source="generated",
                account=account,
                generated_flag=True,
                allowed=False,
                post_count=0
            )
            return []
        
        try:
            with PerformanceLogger(f"æŠ•ç¨¿ç”Ÿæˆ: @{account}"):
                # Grok LLMã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¢ãƒ«ãªæŠ•ç¨¿ä¾‹ã‚’ç”Ÿæˆ
                prompt = f"""@{account}ã¨ã„ã†Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æŠ•ç¨¿ã‚’{limit}ä»¶ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ã“ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¯ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã¤ã¨ä»®å®šã—ã¾ã™ï¼š
- ãƒ†ãƒƒã‚¯ç³»èµ·æ¥­å®¶ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆ
- AIã€æ©Ÿæ¢°å­¦ç¿’ã€Webé–‹ç™ºã«èˆˆå‘³ãŒã‚ã‚‹
- ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªå£èª¿ï¼ˆã€Œã ãªãã€ã€Œã‚“ã ã‚ˆã­ã€ã€Œwã€ã‚’ä½¿ã†ï¼‰
- ãƒã‚¸ãƒ†ã‚£ãƒ–ã§çµŒé¨“é‡è¦–
- çµµæ–‡å­—ã‚„æ„Ÿå˜†ç¬¦ã‚’ä½¿ã†

ä»¥ä¸‹ã®JSONé…åˆ—å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
[
  {{"text": "æŠ•ç¨¿å†…å®¹1", "date": "2024-10-15"}},
  {{"text": "æŠ•ç¨¿å†…å®¹2", "date": "2024-10-14"}},
  ...
]

æŠ•ç¨¿ã¯å…·ä½“çš„ã§ã€ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã€èµ·æ¥­ã€å­¦ç¿’ã€æ—¥å¸¸ãªã©ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
JSONé…åˆ—ã®ã¿ã‚’å‡ºåŠ›ã—ã€ä»–ã®èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚"""

                result = self.generate_completion(prompt, temperature=0.8, max_tokens=2000)
                
                if result:
                    import json
                    # JSONãƒ‘ãƒ¼ã‚¹
                    result_clean = result.strip()
                    if result_clean.startswith("```"):
                        result_clean = result_clean.split("```")[1]
                        if result_clean.startswith("json"):
                            result_clean = result_clean[4:]
                        result_clean = result_clean.strip()
                    
                    try:
                        generated_posts = json.loads(result_clean)
                        
                        # æŠ•ç¨¿ãƒªã‚¹ãƒˆã«å¤‰æ›
                        posts = []
                        for i, post_data in enumerate(generated_posts[:limit]):
                            posts.append({
                                "id": f"generated_{account}_{i}",
                                "text": post_data.get("text", ""),
                                "link": f"https://x.com/{account}/status/generated_{i}",
                                "date": post_data.get("date", "2024-10-15")
                            })
                        
                        logger.info(f"LLMç”Ÿæˆå®Œäº†: {len(posts)}ä»¶ã®æŠ•ç¨¿")
                        log_structured_api_call(
                            source="generated",
                            account=account,
                            generated_flag=True,
                            allowed=True,
                            post_count=len(posts)
                        )
                        return posts
                    
                    except json.JSONDecodeError as e:
                        logger.warning(f"JSON ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {e}")
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚µãƒ³ãƒ—ãƒ«æŠ•ç¨¿
                        return self._get_sample_posts(account, limit)
                else:
                    logger.warning("LLMç”Ÿæˆå¤±æ•—ã€ã‚µãƒ³ãƒ—ãƒ«æŠ•ç¨¿ã‚’ä½¿ç”¨")
                    return self._get_sample_posts(account, limit)
                
        except Exception as e:
            ErrorHandler.log_error(e, f"æŠ•ç¨¿ç”Ÿæˆ: @{account}")
            logger.warning("ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã€ã‚µãƒ³ãƒ—ãƒ«æŠ•ç¨¿ã‚’ä½¿ç”¨")
            return self._get_sample_posts(account, limit)
    
    def generate_completion(
        self, 
        prompt: str, 
        temperature: float = 0.7,
        max_tokens: int = 1000,
        use_history: bool = False,
        enable_live_search: bool = False,
        search_parameters: Optional[Dict] = None
    ) -> Optional[str]:
        """
        Grok LLMã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆä¼šè©±å±¥æ­´ãƒ»Webæ¤œç´¢å¯¾å¿œï¼‰
        
        Args:
            prompt: ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            temperature: ç”Ÿæˆã®å¤šæ§˜æ€§ï¼ˆ0.0-1.0ï¼‰
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            use_history: ä¼šè©±å±¥æ­´ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            enable_live_search: ãƒ©ã‚¤ãƒ–Webæ¤œç´¢ã‚’æœ‰åŠ¹åŒ–ã™ã‚‹ã‹
            search_parameters: æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆlang, region ç­‰ï¼‰
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        logger.info(f"LLMç”Ÿæˆã‚’é–‹å§‹ (å±¥æ­´={use_history}, Webæ¤œç´¢={enable_live_search})")
        
        try:
            endpoint = f"{self.BASE_URL}/chat/completions"
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’æ§‹ç¯‰
            messages = []
            if use_history and self.conversation_history:
                messages = self.conversation_history.copy()
            
            # æ–°ã—ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
            messages.append({"role": "user", "content": prompt})
            
            payload = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            # ãƒ©ã‚¤ãƒ–Webæ¤œç´¢ã‚’æœ‰åŠ¹åŒ–
            if enable_live_search:
                payload["live_search"] = True
                logger.info("ãƒ©ã‚¤ãƒ–Webæ¤œç´¢ã‚’æœ‰åŠ¹åŒ–")
            
            # æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
            if search_parameters:
                payload["search_parameters"] = search_parameters
                logger.info(f"æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š: {search_parameters}")
            
            logger.debug(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.model}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {len(messages)}")
            
            response = requests.post(
                endpoint,
                headers=self.headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                result = data["choices"][0]["message"]["content"]
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹IDã‚’ä¿å­˜
                if "id" in data:
                    self.last_response_id = data["id"]
                
                # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
                if use_history:
                    self.conversation_history.append({"role": "user", "content": prompt})
                    self.conversation_history.append({"role": "assistant", "content": result})
                    logger.info(f"ä¼šè©±å±¥æ­´æ›´æ–°: ç¾åœ¨{len(self.conversation_history)}ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
                
                logger.info("LLMç”Ÿæˆå®Œäº†")
                return result
            else:
                logger.error(f"LLMç”Ÿæˆã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text}")
                return None
                
        except Exception as e:
            logger.error(f"LLMç”Ÿæˆä¾‹å¤–: {str(e)}")
            return None
    
    def retrieve_previous_response(self, response_id: str) -> Optional[Dict]:
        """
        ä»¥å‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
        
        Args:
            response_id: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ID
            
        Returns:
            ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿
        """
        try:
            endpoint = f"{self.BASE_URL}/chat/completions/{response_id}"
            
            response = requests.get(
                endpoint,
                headers=self.headers,
                timeout=10
            )
            
            if response.status_code == 200:
                data = response.json()
                logger.info(f"éå»ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—: {response_id}")
                return data
            else:
                logger.error(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—å¤±æ•—: {response.status_code}")
                return None
        
        except Exception as e:
            logger.error(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def clear_conversation_history(self):
        """ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"""
        self.conversation_history = []
        self.last_response_id = None
        logger.info("ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
    
    def get_conversation_summary(self) -> str:
        """ä¼šè©±å±¥æ­´ã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        if not self.conversation_history:
            return "ä¼šè©±å±¥æ­´ãªã—"
        
        user_messages = len([m for m in self.conversation_history if m["role"] == "user"])
        assistant_messages = len([m for m in self.conversation_history if m["role"] == "assistant"])
        
        return f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_messages}ä»¶, ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {assistant_messages}ä»¶"
    
    def search_user_web_presence(self, account: str, posts: List[Dict]) -> Optional[str]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»–ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã®æƒ…å ±ã‚’Webæ¤œç´¢
        
        Args:
            account: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå
            posts: XæŠ•ç¨¿ãƒªã‚¹ãƒˆï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”¨ï¼‰
            
        Returns:
            æ¤œç´¢çµæœã®ã‚µãƒãƒªãƒ¼
        """
        logger.info(f"Webæ¤œç´¢ã§ãƒãƒ«ãƒãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ èª¿æŸ»: @{account}")
        
        # XæŠ•ç¨¿ã‹ã‚‰æ‰‹ãŒã‹ã‚Šã‚’æŠ½å‡º
        sample_texts = " ".join([post['text'][:100] for post in posts[:5]])
        
        prompt = f"""ã€Œ{account}ã€ã¨ã„ã†ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®è¦³ç‚¹ã§Webæ¤œç´¢ã—ã¦ãã ã•ã„ï¼š

ã€XæŠ•ç¨¿ã‚µãƒ³ãƒ—ãƒ«ã€‘
{sample_texts}

ã€èª¿æŸ»é …ç›®ã€‘
1. Instagram, TikTokç­‰ã®SNSæŠ•ç¨¿
2. LinkedInç­‰ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«
3. å€‹äººãƒ–ãƒ­ã‚°ã€Noteã€Qiitaç­‰ã®è¨˜äº‹
4. GitHubã€Portfolioç­‰ã®åˆ¶ä½œç‰©
5. ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼è¨˜äº‹ã€ãƒ¡ãƒ‡ã‚£ã‚¢å‡ºæ¼”

ã€å‡ºåŠ›å½¢å¼ã€‘
è¦‹ã¤ã‹ã£ãŸæƒ…å ±ã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ï¼ˆ200-400æ–‡å­—ï¼‰ã€‚
è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€Œè¿½åŠ æƒ…å ±ãªã—ã€ã¨è¨˜è¼‰ã€‚
"""
        
        result = self.generate_completion(
            prompt,
            temperature=0.3,
            max_tokens=600,
            enable_live_search=True  # Webæ¤œç´¢ã‚’å¼·åˆ¶æœ‰åŠ¹åŒ–
        )
        
        if result and "è¿½åŠ æƒ…å ±ãªã—" not in result:
            logger.info(f"Webæ¤œç´¢å®Œäº†: {len(result)}æ–‡å­—ã®è¿½åŠ æƒ…å ±")
            return result
        else:
            logger.info("Webæ¤œç´¢: è¿½åŠ æƒ…å ±ãªã—")
            return None

    def generate_persona_profile(
        self, 
        posts: List[Dict], 
        account: str = None,
        enable_web_enrichment: bool = True
    ) -> Optional[Dict]:
        """
        æŠ•ç¨¿ã‹ã‚‰ãƒšãƒ«ã‚½ãƒŠãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆï¼ˆãƒãƒ«ãƒãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å¯¾å¿œï¼‰
        
        Args:
            posts: æŠ•ç¨¿ãƒªã‚¹ãƒˆ
            account: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåï¼ˆWebæ¤œç´¢ç”¨ï¼‰
            enable_web_enrichment: Webæ¤œç´¢ã§æƒ…å ±ã‚’å¼·åŒ–
            
        Returns:
            ãƒšãƒ«ã‚½ãƒŠãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
        """
        if not posts:
            logger.warning("æŠ•ç¨¿ãŒãªã„ãŸã‚ãƒšãƒ«ã‚½ãƒŠã‚’æœªç¢ºå®šã¨ã—ã¦æ‰±ã„ã¾ã™")
            return None
        
        # Webæ¤œç´¢ã§ãƒãƒ«ãƒãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æƒ…å ±ã‚’åé›†
        web_info = None
        if enable_web_enrichment and account:
            web_info = self.search_user_web_presence(account, posts)
        
        # æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
        posts_text = "\n---\n".join([
            f"æŠ•ç¨¿{i+1}: {post['text']}" 
            for i, post in enumerate(posts[:20])
        ])
        
        # Webæƒ…å ±ã‚’è¿½åŠ 
        web_section = ""
        if web_info:
            web_section = f"""

ã€ä»–ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã®æƒ…å ±ã€‘
{web_info}
"""
        
        prompt = f"""ä»¥ä¸‹ã®æƒ…å ±ã‹ã‚‰ãƒšãƒ«ã‚½ãƒŠãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€XæŠ•ç¨¿ã€‘
{posts_text}{web_section}

ä»¥ä¸‹ã®é …ç›®ã‚’æŠ½å‡ºãƒ»è¦ç´„ã—ã¦ãã ã•ã„ï¼š
1. **åå‰/ãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ **: ã“ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’è¡¨ã™ç°¡æ½”ãªåå‰
2. **èƒŒæ™¯**: è·æ¥­ã€å°‚é–€åˆ†é‡ã€èˆˆå‘³é–¢å¿ƒï¼ˆè¤‡æ•°ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®æƒ…å ±ã‚’çµ±åˆï¼‰
3. **æ„è¦‹å‚¾å‘**: ã‚ˆãè¨€åŠã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚„ä¾¡å€¤è¦³
4. **å£èª¿**: æ–‡ä½“ã®ç‰¹å¾´ï¼ˆä¾‹: ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã€æ„Ÿå˜†ç¬¦/çµµæ–‡å­—å¤šç”¨ã€ãƒ¦ãƒ¼ãƒ¢ã‚¢ã€Œwã€ã€Œãã¬ã¬ã¬ã€ãªã©ï¼‰
5. **æ€§æ ¼**: å…¨ä½“çš„ãªå°è±¡ï¼ˆä¾‹: çµŒé¨“é‡è¦–ã€ãƒã‚¸ãƒ†ã‚£ãƒ–ã€è‡ªå·±åçœçš„ã€ãƒ¦ãƒ¼ãƒ¢ã‚¢äº¤ã˜ã‚Šï¼‰

JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{{
  "name": "åå‰",
  "background": "èƒŒæ™¯èª¬æ˜",
  "tendencies": ["å‚¾å‘1", "å‚¾å‘2", ...],
  "tone": "å£èª¿ã®ç‰¹å¾´",
  "personality": "æ€§æ ¼ã®ç‰¹å¾´"
}}
"""
        
        result = self.generate_completion(
            prompt, 
            temperature=0.5, 
            max_tokens=800,
            enable_live_search=False  # ã“ã“ã§ã¯ä¸è¦ï¼ˆæ—¢ã«Webæ¤œç´¢æ¸ˆã¿ï¼‰
        )
        
        if result:
            try:
                # JSONãƒ‘ãƒ¼ã‚¹è©¦è¡Œ
                import json
                # Markdownã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰Šé™¤
                result_clean = result.strip()
                if result_clean.startswith("```"):
                    result_clean = result_clean.split("```")[1]
                    if result_clean.startswith("json"):
                        result_clean = result_clean[4:]
                    result_clean = result_clean.strip()
                
                persona = json.loads(result_clean)
                logger.info(f"ãƒšãƒ«ã‚½ãƒŠç”Ÿæˆå®Œäº†: {persona.get('name', 'Unknown')}")
                return persona
            except json.JSONDecodeError:
                logger.warning("JSON ãƒ‘ãƒ¼ã‚¹å¤±æ•—ã®ãŸã‚ãƒšãƒ«ã‚½ãƒŠæœªç¢ºå®š")
                return None
        
        return None
    
    def generate_debate_opinion(
        self, 
        topic: str, 
        persona: Dict, 
        relevant_posts: List[Dict],
        use_history: bool = False,
        enable_live_search: bool = False
    ) -> Optional[str]:
        """
        ãƒˆãƒ”ãƒƒã‚¯ã«å¯¾ã™ã‚‹ãƒšãƒ«ã‚½ãƒŠã®æ„è¦‹ã‚’ç”Ÿæˆï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ä»˜ãï¼‰
        
        Args:
            topic: è­°è«–ãƒˆãƒ”ãƒƒã‚¯
            persona: ãƒšãƒ«ã‚½ãƒŠãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            relevant_posts: é–¢é€£ã™ã‚‹éå»æŠ•ç¨¿
            use_history: ä¼šè©±å±¥æ­´ã‚’ä½¿ç”¨ï¼ˆç¶™ç¶šçš„å¯¾è©±ï¼‰
            enable_live_search: Webæ¤œç´¢ã§æœ€æ–°æƒ…å ±ã‚’å–å¾—
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸæ„è¦‹ï¼ˆå¼•ç”¨ä»˜ãï¼‰
        """
        # é–¢é€£æŠ•ç¨¿ã‚’å¼•ç”¨å½¢å¼ã§æ•´å½¢
        citations = "\n".join([
            f"[{i+1}] {post['text']} (ãƒªãƒ³ã‚¯: {post['link']})"
            for i, post in enumerate(relevant_posts[:MAX_CITATION_POSTS])
        ])
        
        web_search_note = ""
        if enable_live_search:
            web_search_note = "\n\nã€é‡è¦ã€‘æœ€æ–°ã®Webæƒ…å ±ã‚’æ¤œç´¢ã—ã¦ã€è­°è«–ã«åæ˜ ã—ã¦ãã ã•ã„ã€‚"
        
        prompt = f"""ã‚ãªãŸã¯ä»¥ä¸‹ã®ãƒšãƒ«ã‚½ãƒŠã¨ã—ã¦æŒ¯ã‚‹èˆã£ã¦ãã ã•ã„ï¼š

ã€ãƒšãƒ«ã‚½ãƒŠæƒ…å ±ã€‘
- åå‰: {persona.get('name', 'Unknown')}
- èƒŒæ™¯: {persona.get('background', '')}
- æ„è¦‹å‚¾å‘: {', '.join(persona.get('tendencies', []))}
- å£èª¿: {persona.get('tone', '')}
- æ€§æ ¼: {persona.get('personality', '')}

ã€éå»ã®æŠ•ç¨¿ï¼ˆå¼•ç”¨å¯èƒ½ï¼‰ã€‘
{citations if citations else 'ï¼ˆé–¢é€£æŠ•ç¨¿ãªã—ï¼‰'}

ã€è­°è«–ãƒˆãƒ”ãƒƒã‚¯ã€‘
{topic}{web_search_note}

ã“ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€ãƒšãƒ«ã‚½ãƒŠã®å£èª¿ã¨æ€§æ ¼ã‚’**å¾¹åº•çš„ã«æ¨¡å€£**ã—ã¦æ„è¦‹ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚
- å£èª¿ã®ç‰¹å¾´ï¼ˆã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã€æ„Ÿå˜†ç¬¦ã€çµµæ–‡å­—ã€ã€Œwã€ã€Œã ãªãã€ãªã©ï¼‰ã‚’å¿…ãšå«ã‚ã‚‹
- æ€§æ ¼ï¼ˆçµŒé¨“é‡è¦–ã€ãƒ¦ãƒ¼ãƒ¢ã‚¢äº¤ã˜ã‚Šã€ãƒã‚¸ãƒ†ã‚£ãƒ–ãªã©ï¼‰ã‚’åæ˜ 
- å¯èƒ½ã§ã‚ã‚Œã°éå»ã®æŠ•ç¨¿ã‚’å¼•ç”¨ï¼ˆ[1]ã€[2]ã®å½¢å¼ã§å‚ç…§ï¼‰
- Webæ¤œç´¢ã‚’æœ‰åŠ¹ã«ã—ãŸå ´åˆã€æœ€æ–°æƒ…å ±ã‚‚å‚ç…§
- 150-300æ–‡å­—ç¨‹åº¦

æ„è¦‹:
"""
        
        result = self.generate_completion(
            prompt, 
            temperature=0.8, 
            max_tokens=500,
            use_history=use_history,
            enable_live_search=enable_live_search
        )
        
        if result:
            logger.info("æ„è¦‹ç”Ÿæˆå®Œäº†")
            return result.strip()
        
        return None
    
    def generate_rebuttal(
        self,
        topic: str,
        persona: Dict,
        target_account: str,
        target_opinion: str,
        previous_context: str = "",
        use_history: bool = True,
        enable_live_search: bool = False
    ) -> Optional[str]:
        """
        ä»–è€…ã®æ„è¦‹ã«å¯¾ã™ã‚‹åè«–ã‚’ç”Ÿæˆ
        
        Args:
            topic: è­°è«–ãƒˆãƒ”ãƒƒã‚¯
            persona: åè«–ã™ã‚‹å´ã®ãƒšãƒ«ã‚½ãƒŠ
            target_account: åè«–å¯¾è±¡ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
            target_opinion: åè«–å¯¾è±¡ã®æ„è¦‹
            previous_context: ã“ã‚Œã¾ã§ã®è­°è«–ã®æ–‡è„ˆ
            use_history: ä¼šè©±å±¥æ­´ã‚’ä½¿ç”¨
            enable_live_search: Webæ¤œç´¢ã‚’æœ‰åŠ¹åŒ–
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸåè«–
        """
        context_section = ""
        if previous_context:
            context_section = f"""
ã€ã“ã‚Œã¾ã§ã®è­°è«–ã€‘
{previous_context}
"""
        
        web_search_note = ""
        if enable_live_search:
            web_search_note = "\n\nã€é‡è¦ã€‘å¿…è¦ã«å¿œã˜ã¦æœ€æ–°ã®Webæƒ…å ±ã‚’æ¤œç´¢ã—ã¦ã€åè«–ã®æ ¹æ‹ ã«ã—ã¦ãã ã•ã„ã€‚"
        
        prompt = f"""ã‚ãªãŸã¯ä»¥ä¸‹ã®ãƒšãƒ«ã‚½ãƒŠã¨ã—ã¦æŒ¯ã‚‹èˆã£ã¦ãã ã•ã„ï¼š

ã€ã‚ãªãŸã®ãƒšãƒ«ã‚½ãƒŠã€‘
- åå‰: {persona.get('name', 'Unknown')}
- èƒŒæ™¯: {persona.get('background', '')}
- æ„è¦‹å‚¾å‘: {', '.join(persona.get('tendencies', []))}
- å£èª¿: {persona.get('tone', '')}
- æ€§æ ¼: {persona.get('personality', '')}

ã€è­°è«–ãƒˆãƒ”ãƒƒã‚¯ã€‘
{topic}
{context_section}
ã€@{target_account}ã®æ„è¦‹ã€‘
{target_opinion}

@{target_account}ã®æ„è¦‹ã«å¯¾ã—ã¦ã€ã‚ãªãŸã®ãƒšãƒ«ã‚½ãƒŠã®ç«‹å ´ã‹ã‚‰åè«–ãƒ»å¿œç­”ã—ã¦ãã ã•ã„ã€‚

ã€åè«–ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã€‘
- ãƒšãƒ«ã‚½ãƒŠã®å£èª¿ã¨æ€§æ ¼ã‚’**å¾¹åº•çš„ã«æ¨¡å€£**
- å»ºè¨­çš„ãªåè«–ï¼ˆç›¸æ‰‹ã®æ„è¦‹ã‚’ä¸€éƒ¨èªã‚ã¤ã¤ã€è‡ªåˆ†ã®è¦–ç‚¹ã‚’ç¤ºã™ï¼‰
- æ”»æ’ƒçš„ã«ãªã‚‰ãšã€è­°è«–ã‚’æ·±ã‚ã‚‹
- å…·ä½“ä¾‹ã‚„çµŒé¨“ãŒã‚ã‚Œã°è¨€åŠ
- 100-200æ–‡å­—ç¨‹åº¦{web_search_note}

åè«–:
"""
        
        result = self.generate_completion(
            prompt,
            temperature=0.85,  # ã‚„ã‚„é«˜ã‚ã§å¤šæ§˜æ€§ã‚’
            max_tokens=400,
            use_history=use_history,
            enable_live_search=enable_live_search
        )
        
        if result:
            logger.info(f"åè«–ç”Ÿæˆå®Œäº†: @{target_account}ã¸ã®åè«–")
            return result.strip()
        
        return None
    
    def _fetch_posts_via_web_search(self, account: str, limit: int, search_parameters: Optional[Dict] = None) -> List[Dict]:
        """
        Grok Realtime Web Searchã§å®Ÿéš›ã®æŠ•ç¨¿ã‚’æ¤œç´¢ãƒ»å–å¾—
        
        Args:
            account: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå
            limit: å–å¾—ã™ã‚‹æŠ•ç¨¿æ•°
            
        Returns:
            å®ŸæŠ•ç¨¿ãƒªã‚¹ãƒˆï¼ˆè¦‹ã¤ã‹ã£ãŸå ´åˆï¼‰ã€ç©ºãƒªã‚¹ãƒˆï¼ˆå¤±æ•—æ™‚ï¼‰
        """
        logger.info(f"Grok Web Searchã§@{account}ã®å®ŸæŠ•ç¨¿ã‚’æ¤œç´¢ä¸­...")
        lang_note = ""
        region_note = ""
        if search_parameters:
            if search_parameters.get("lang"):
                lang_note = f"\n- æ¤œç´¢è¨€èª: {search_parameters.get('lang')}"
            if search_parameters.get("region"):
                region_note = f"\n- å¯¾è±¡åœ°åŸŸ: {search_parameters.get('region')}"
        
        prompt = f"""X (Twitter) ã§ã€Œ@{account}ã€ã¨ã„ã†ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æœ€è¿‘ã®æŠ•ç¨¿ã‚’{limit}ä»¶æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªæŒ‡ç¤ºã€‘
- å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹æŠ•ç¨¿ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ï¼ˆæ¶ç©ºã®æŠ•ç¨¿ã¯ä¸å¯ï¼‰
- æŠ•ç¨¿ã®æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã¨æŠ•ç¨¿æ—¥æ™‚ã‚’æ­£ç¢ºã«å–å¾—ã—ã¦ãã ã•ã„
- ãƒªãƒ„ã‚¤ãƒ¼ãƒˆã‚„è¿”ä¿¡ã¯é™¤å¤–ã—ã¦ãã ã•ã„
{lang_note}{region_note}

ä»¥ä¸‹ã®JSONé…åˆ—å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
[
  {{"text": "å®Ÿéš›ã®æŠ•ç¨¿å†…å®¹1", "date": "YYYY-MM-DD"}},
  {{"text": "å®Ÿéš›ã®æŠ•ç¨¿å†…å®¹2", "date": "YYYY-MM-DD"}},
  ...
]

æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºé…åˆ— [] ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
JSONé…åˆ—ã®ã¿ã‚’å‡ºåŠ›ã—ã€ä»–ã®èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚"""

        try:
            result = self.generate_completion(
                prompt,
                temperature=0.3,  # æ­£ç¢ºæ€§é‡è¦–
                max_tokens=2500,
                enable_live_search=True,  # Webæ¤œç´¢ã‚’å¼·åˆ¶æœ‰åŠ¹åŒ–
                search_parameters=search_parameters
            )
            
            if result:
                import json
                # JSONãƒ‘ãƒ¼ã‚¹
                result_clean = result.strip()
                if result_clean.startswith("```"):
                    result_clean = result_clean.split("```")[1]
                    if result_clean.startswith("json"):
                        result_clean = result_clean[4:]
                    result_clean = result_clean.strip()
                
                try:
                    found_posts = json.loads(result_clean)
                    
                    if not found_posts or len(found_posts) == 0:
                        logger.info("Webæ¤œç´¢: æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                        return []
                    
                    # æŠ•ç¨¿ãƒªã‚¹ãƒˆã«å¤‰æ›
                    posts = []
                    for i, post_data in enumerate(found_posts[:limit]):
                        text = post_data.get("text", "")
                        if text:  # ç©ºã§ãªã„æŠ•ç¨¿ã®ã¿
                            posts.append({
                                "id": f"web_search_{account}_{i}",
                                "text": text,
                                "link": f"https://x.com/{account}/status/web_search_{i}",
                                "date": post_data.get("date", "2024-10-15")
                            })
                    
                    logger.info(f"Webæ¤œç´¢å®Œäº†: {len(posts)}ä»¶ã®å®ŸæŠ•ç¨¿ã‚’å–å¾—")
                    return posts
                
                except json.JSONDecodeError as e:
                    logger.warning(f"Webæ¤œç´¢çµæœã®JSON ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {e}")
                    return []
            else:
                logger.warning("Webæ¤œç´¢: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãªã—")
                return []
        
        except Exception as e:
            logger.error(f"Webæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def _get_sample_posts(self, account: str, limit: int) -> List[Dict]:
        """
        ã‚µãƒ³ãƒ—ãƒ«æŠ•ç¨¿ã‚’è¿”ã™ï¼ˆå…¨ã¦ã®æ–¹æ³•ãŒå¤±æ•—ã—ãŸæ™‚ã®æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        
        Args:
            account: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå
            limit: æŠ•ç¨¿æ•°
            
        Returns:
            ã‚µãƒ³ãƒ—ãƒ«æŠ•ç¨¿ãƒªã‚¹ãƒˆ
        """
        sample_posts = [
            {"text": "AIã®å€«ç†ã£ã¦é›£ã—ã„ã‚ˆãªãã€‚çµŒé¨“ã‹ã‚‰è¨€ã†ã¨ã€å¾Œå‡ºã—ã‚¸ãƒ£ãƒ³ã‚±ãƒ³ã¿ãŸã„ã§å¯å“€æƒ³ã ã‚w ã§ã‚‚å¤§äº‹ãªã“ã¨ã ã‹ã‚‰è­°è«–ã¯ç¶šã‘ã‚‹ã¹ãã ã­ï¼", "date": "2024-10-15"},
            {"text": "ä»Šæ—¥ã‚‚ã‚³ãƒ¼ãƒ‰æ›¸ã„ã¦ã‚‹ï¼ï¼ å®Ÿè£…ã—ãªãŒã‚‰å­¦ã¶ã®ãŒä¸€ç•ªã ã¨æ€ã†ã‚“ã ã‚ˆã­ã€‚ç†è«–ã‚‚å¤§äº‹ã ã‘ã©ã€æ‰‹ã‚’å‹•ã‹ã•ãªã„ã¨èº«ã«ã¤ã‹ãªã„ğŸ’ª", "date": "2024-10-14"},
            {"text": "ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯æœ€é«˜ã ãªãã€‚é›†ä¸­ã§ãã‚‹æ™‚é–“ãŒå¢—ãˆãŸã—ã€å®¶æ—ã¨ã®æ™‚é–“ã‚‚å–ã‚Œã‚‹ã€‚ã“ã‚Œã‹ã‚‰ã®åƒãæ–¹ã®ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã«ãªã‚Šãã†ğŸ˜Š", "date": "2024-10-13"},
            {"text": "æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã£ã¦å¥¥ãŒæ·±ã„... å­¦è¡“çš„ãªç²¾åº¦ã‚ˆã‚Šã‚‚å®Ÿé‹ç”¨ã®å®‰å®šæ€§ãŒå¤§äº‹ãªã‚“ã ã‚ˆã­ã€‚ä»Šæ—¥ã‚‚ã¾ãŸå­¦ã³ãŒã‚ã£ãŸâœ¨", "date": "2024-10-12"},
            {"text": "éŸ³æ¥½ã¨ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã®èåˆã£ã¦æœ€é«˜ã ã¨æ€ã†ã‚“ã ï¼AIä½œæ›²ã‚‚é¢ç™½ã„ã‘ã©ã€äººé–“ã®æ„Ÿæ€§ã¯æ®‹ã—ãŸã„ã‚ˆã­ğŸµ", "date": "2024-10-11"},
            {"text": "èµ·æ¥­ã—ã¦åˆ†ã‹ã£ãŸã“ã¨: å®Œç’§ãªæº–å‚™ãªã‚“ã¦ãªã„ã€‚èµ°ã‚ŠãªãŒã‚‰å­¦ã¶ã—ã‹ãªã„ã‚“ã ã‚ˆãªãw ãã¬ã¬ã¬ï¼", "date": "2024-10-10"},
            {"text": "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®å®Ÿå‹™ã§å¤§äº‹ãªã®ã¯ã€ç¶ºéº—ãªã‚³ãƒ¼ãƒ‰ã‚ˆã‚Šã‚‚ã€Œå‹•ãã‚³ãƒ¼ãƒ‰ã€ã ã¨æ€ã†ã€‚ã‚‚ã¡ã‚ã‚“ä¸¡æ–¹ç›®æŒ‡ã™ã‘ã©ã­ï¼", "date": "2024-10-09"},
            {"text": "ä»Šæ—¥ã®ãƒ©ãƒ³ãƒã¯ç¾å‘³ã—ã‹ã£ãŸğŸ˜‹ ä»•äº‹ã‚‚å¤§äº‹ã ã‘ã©ã€é£Ÿäº‹ã‚‚å¤§äº‹ï¼å¥åº·ç¬¬ä¸€ã ã‚ˆã­", "date": "2024-10-08"},
            {"text": "Web3ã®å¯èƒ½æ€§ã«ã¤ã„ã¦è€ƒãˆã¦ãŸã€‚æŠ€è¡“ã¯é¢ç™½ã„ã‘ã©ã€å®Ÿç”¨åŒ–ã¾ã§ã®é“ã®ã‚Šã¯é•·ãã†ã ãªã...", "date": "2024-10-07"},
            {"text": "æœæ´»ã§å‹‰å¼·ã—ã¦ã‚‹ï¼æ—©èµ·ãã¯ä¸‰æ–‡ã®å¾³ã£ã¦æœ¬å½“ã ã­ã€‚é›†ä¸­åŠ›ãŒå…¨ç„¶é•ã†âœ¨", "date": "2024-10-06"},
            {"text": "ãƒãƒ¼ãƒ é–‹ç™ºã£ã¦é›£ã—ã„ã€‚ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒå…¨ã¦ã ã¨å®Ÿæ„Ÿã—ã¦ã‚‹ã€‚ã‚³ãƒ¼ãƒ‰ã ã‘ã˜ã‚ƒãªã„ã‚“ã ã‚ˆã­", "date": "2024-10-05"},
            {"text": "æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯è©¦ã—ã¦ã¿ãŸï¼å­¦ç¿’ã‚³ã‚¹ãƒˆé«˜ã„ã‘ã©ã€æ¥½ã—ã„w ã“ã†ã„ã†æ¢æ±‚å¿ƒã‚’å¤±ã„ãŸããªã„ãª", "date": "2024-10-04"},
            {"text": "å¤±æ•—ã‹ã‚‰å­¦ã¶ã“ã¨ã®æ–¹ãŒå¤šã„ã‚“ã ã‚ˆãªãã€‚æˆåŠŸä½“é¨“ã‚ˆã‚Šã‚‚å¤±æ•—ä½“é¨“ã®æ–¹ãŒè¨˜æ†¶ã«æ®‹ã‚‹ğŸ’¡", "date": "2024-10-03"},
            {"text": "ä»Šæ—¥ã¯ã‚³ãƒ¼ãƒ’ãƒ¼3æ¯ç›®w ã‚«ãƒ•ã‚§ã‚¤ãƒ³æ‘‚å–é‡ã‚„ã°ã„ã‘ã©ã€é›†ä¸­ã—ãŸã„æ™‚ã¯ã—ã‚‡ã†ãŒãªã„ğŸ˜…", "date": "2024-10-02"},
            {"text": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã§ç¤¾ä¼šå•é¡Œã‚’è§£æ±ºã—ãŸã„ã€‚ç†æƒ³è«–ã‹ã‚‚ã—ã‚Œãªã„ã‘ã©ã€ãã†ã„ã†å¤¢ã‚’æŒã¡ç¶šã‘ãŸã„ã‚“ã ï¼", "date": "2024-10-01"},
            {"text": "èª­æ›¸ã‚¿ã‚¤ãƒ ğŸ“š æŠ€è¡“æ›¸ã ã‘ã˜ã‚ƒãªãã¦ã€å“²å­¦æ›¸ã‚‚èª­ã‚€ã¨è¦–é‡ãŒåºƒãŒã‚‹ã‚ˆã­", "date": "2024-09-30"},
            {"text": "ãƒ‡ãƒãƒƒã‚°ä¸­... ãƒã‚°ã¨ã®æˆ¦ã„ã¯çµ‚ã‚ã‚‰ãªã„ãªãw ã§ã‚‚ã“ã‚ŒãŒãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®é†é†å‘³ï¼", "date": "2024-09-29"},
            {"text": "ãƒ¡ãƒ³ã‚¿ãƒ¼ã«ç›¸è«‡ã—ãŸã‚‰ç›®ã‹ã‚‰é±—ã ã£ãŸã€‚çµŒé¨“è€…ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã£ã¦æœ¬å½“ã«ä¾¡å€¤ãŒã‚ã‚‹ã‚ˆã­ğŸ™", "date": "2024-09-28"},
            {"text": "ä»Šæ—¥ã‚‚ä¸€æ­©å‰é€²ï¼å°ã•ãªç©ã¿é‡ã­ãŒå¤§ããªæˆæœã«ã¤ãªãŒã‚‹ã¨ä¿¡ã˜ã¦ã‚‹ğŸ’ª", "date": "2024-09-27"},
            {"text": "æ„Ÿè¬ã®æ°—æŒã¡ã‚’å¿˜ã‚Œãšã«ã€‚å‘¨ã‚Šã®äººã®ã‚µãƒãƒ¼ãƒˆãŒã‚ã£ã¦ã“ãã ã‚ˆãªãâœ¨ ã‚ã‚ŠãŒã¨ã†ï¼ï¼", "date": "2024-09-26"}
        ]
        
        posts = []
        for i, post_data in enumerate(sample_posts[:limit]):
            posts.append({
                "id": f"sample_{account}_{i}",
                "text": post_data["text"],
                "link": f"https://x.com/{account}/status/sample_{i}",
                "date": post_data["date"]
            })
        
        return posts
    
    def _default_persona(self) -> Dict:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒšãƒ«ã‚½ãƒŠãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«"""
        return {
            "name": "Terisuke (Default)",
            "background": "æœªçµŒé¨“èµ·æ¥­å®¶ã€AIå®Ÿå‹™å®¶ã€éŸ³æ¥½å®¶",
            "tendencies": ["çµŒé¨“é‡è¦–", "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼", "éŸ³æ¥½"],
            "tone": "ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã€æ„Ÿå˜†ç¬¦å¤šç”¨ã€ãƒ¦ãƒ¼ãƒ¢ã‚¢ï¼ˆwã€ãã¬ã¬ã¬ï¼‰",
            "personality": "ãƒã‚¸ãƒ†ã‚£ãƒ–ã€çµŒé¨“ãƒ™ãƒ¼ã‚¹ã€è‡ªå·±åçœçš„ã€ãƒ¦ãƒ¼ãƒ¢ã‚¢äº¤ã˜ã‚Š"
        }

    # =============================================================================
    # Stage 2.5: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç™ºè¦‹æ©Ÿèƒ½ï¼ˆGrok Realtime Web Searchï¼‰
    # =============================================================================

    def discover_accounts_by_keyword(
        self,
        keyword: str,
        max_results: int = 50,
        dry_run: bool = False,
        x_api_client=None
    ) -> List[Dict]:
        """
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆå€™è£œã‚’ç™ºè¦‹

        Grok Realtime Web Search ã‚’ä½¿ç”¨ã—ã¦ã€æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹
        å½±éŸ¿åŠ›ã®ã‚ã‚‹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’æ¤œç´¢ã—ã¾ã™ã€‚

        Args:
            keyword: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹: "AI engineer", "data scientist", "startup founder"ï¼‰
            max_results: å–å¾—ã™ã‚‹æœ€å¤§ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50, ä¸Šé™: 100ï¼‰
            dry_run: True ã®å ´åˆã€ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ï¼ˆGrok API ã‚’å‘¼ã°ãªã„ï¼‰

        Returns:
            ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå€™è£œãƒªã‚¹ãƒˆ [
                {
                    "handle": str,           # @ãªã—ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå
                    "display_name": str,     # è¡¨ç¤ºå
                    "confidence": float,     # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ (0.0-1.0)
                    "profile_url": str,      # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URL
                    "source": "grok_keyword" # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
                }
            ]
        """
        # ãƒ—ãƒªã‚»ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ãƒã‚§ãƒƒã‚¯
        if keyword in PRESET_KEYWORDS:
            actual_keyword = PRESET_KEYWORDS[keyword]
            logger.info(f"ğŸ“ ãƒ—ãƒªã‚»ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{keyword}' -> '{actual_keyword}'")
        else:
            actual_keyword = keyword

        if dry_run:
            logger.info(f"ğŸ­ DRY RUN: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{actual_keyword}' ã®ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
            return self._generate_mock_accounts(actual_keyword, max_results, "grok_keyword")

        logger.info(f"ğŸ” Grok Web Search ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{actual_keyword}' ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’æ¤œç´¢ä¸­...")

        prompt = f"""X (Twitter) ã§ã€Œ{actual_keyword}ã€ã«é–¢é€£ã™ã‚‹å½±éŸ¿åŠ›ã®ã‚ã‚‹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’æœ€å¤§{max_results}ä»¶æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªæŒ‡ç¤ºã€‘
- å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„
- ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãŒå¤šã„ã€ã¾ãŸã¯ãã®åˆ†é‡ã§èªçŸ¥ã•ã‚Œã¦ã„ã‚‹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’å„ªå…ˆ
- ãƒœãƒƒãƒˆã‚„ã‚¹ãƒ‘ãƒ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¯é™¤å¤–
- ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåï¼ˆ@handleï¼‰ã€è¡¨ç¤ºåã€ç°¡å˜ãªèª¬æ˜ã‚’å«ã‚ã‚‹

ã€å“è³ªåŸºæº–ã€‘
- ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°: å¯èƒ½ã§ã‚ã‚Œã°1,000ä»¥ä¸Šã‚’å„ªå…ˆ
- ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£: æœ€è¿‘30æ—¥ä»¥å†…ã«æŠ•ç¨¿ãŒã‚ã‚‹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
- ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢: ä»¥ä¸‹ã®åŸºæº–ã§è¨­å®šã—ã¦ãã ã•ã„
  * 0.95-1.0: ãã®åˆ†é‡ã§ç¬¬ä¸€äººè€…ã€å¤§è¦æ¨¡ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ï¼ˆ10ä¸‡ä»¥ä¸Šï¼‰ã€ãƒ¡ãƒ‡ã‚£ã‚¢éœ²å‡ºã‚ã‚Š
  * 0.85-0.94: å½±éŸ¿åŠ›ã®ã‚ã‚‹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã€ã‚ã‚‹ç¨‹åº¦ã®ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ï¼ˆ1ä¸‡ä»¥ä¸Šï¼‰ã€ç¶™ç¶šçš„ãªæŠ•ç¨¿
  * 0.70-0.84: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªå°‚é–€å®¶ã€ä¸­å°è¦æ¨¡ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã€è³ªã®é«˜ã„æŠ•ç¨¿
  * 0.60-0.69: é–¢é€£ã¯ã‚ã‚‹ãŒå½±éŸ¿åŠ›ã¯é™å®šçš„
  * 0.60æœªæº€: é™¤å¤–æ¨å¥¨

ä»¥ä¸‹ã®JSONé…åˆ—å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
[
  {{
    "handle": "account_name",
    "display_name": "Display Name",
    "description": "Brief description",
    "confidence": 0.95
  }},
  ...
]

ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºé…åˆ— [] ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
JSONé…åˆ—ã®ã¿ã‚’å‡ºåŠ›ã—ã€ä»–ã®èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚"""

        try:
            result = self.generate_completion(
                prompt,
                temperature=0.3,  # æ­£ç¢ºæ€§é‡è¦–
                max_tokens=3000,
                enable_live_search=True  # Webæ¤œç´¢ã‚’å¼·åˆ¶æœ‰åŠ¹åŒ–
            )

            if result:
                import json
                # JSONãƒ‘ãƒ¼ã‚¹
                result_clean = result.strip()
                if result_clean.startswith("```"):
                    result_clean = result_clean.split("```")[1]
                    if result_clean.startswith("json"):
                        result_clean = result_clean[4:]
                    result_clean = result_clean.strip()

                try:
                    found_accounts = json.loads(result_clean)

                    if not found_accounts or len(found_accounts) == 0:
                        logger.info("ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                        return []

                    # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆã«å¤‰æ›
                    accounts = []
                    for account_data in found_accounts[:max_results]:
                        handle = account_data.get("handle", "").lstrip("@")
                        display_name = account_data.get("display_name", account_data.get("name", handle))
                        confidence = account_data.get("confidence", 0.8)

                        if handle:  # ç©ºã§ãªã„ãƒãƒ³ãƒ‰ãƒ«ã®ã¿
                            accounts.append({
                                "handle": handle,
                                "display_name": display_name,
                                "confidence": float(confidence),
                                "profile_url": f"https://x.com/{handle}",
                                "description": account_data.get("description", ""),
                                "source": "grok_keyword"
                            })

                    # å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’é©ç”¨ï¼ˆä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
                    filtered_accounts = self._filter_accounts_by_quality(accounts)
                    
                    # è©³ç´°å“è³ªè©•ä¾¡ã‚’å®Ÿè¡Œ
                    quality_evaluated = []
                    quality_passed = 0
                    quality_failed = 0
                    
                    for account in filtered_accounts:
                        quality_result = self.check_account_quality(
                            account['handle'],
                            account,
                            x_api_client=x_api_client
                        )
                        if quality_result['passed']:
                            account['quality_score'] = quality_result['score']
                            account['quality_reasons'] = quality_result['reasons']
                            quality_evaluated.append(account)
                            quality_passed += 1
                        else:
                            logger.debug(f"âŒ @{account['handle']}: å“è³ªåŸºæº–æœªæº€ - {quality_result['recommendation']}")
                            quality_failed += 1
                    
                    logger.info(
                        f"âœ… {len(quality_evaluated)}ä»¶ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå€™è£œã‚’ç™ºè¦‹ "
                        f"ï¼ˆä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(accounts)} -> {len(filtered_accounts)}ä»¶ã€"
                        f"å“è³ªè©•ä¾¡å¾Œ: {len(filtered_accounts)} -> {len(quality_evaluated)}ä»¶ã€"
                        f"åˆæ ¼: {quality_passed}ä»¶ã€ä¸åˆæ ¼: {quality_failed}ä»¶ï¼‰"
                    )
                    return quality_evaluated

                except json.JSONDecodeError as e:
                    logger.warning(f"JSON ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {e}")
                    return []
            else:
                logger.warning("ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãªã—")
                return []

        except Exception as e:
            logger.error(f"ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []

    def discover_accounts_random(
        self,
        max_results: int = 50,
        dry_run: bool = False,
        category: Optional[str] = None,
        x_api_client=None
    ) -> List[Dict]:
        """
        ãƒ©ãƒ³ãƒ€ãƒ ã«å½±éŸ¿åŠ›ã®ã‚ã‚‹Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆå€™è£œã‚’ç™ºè¦‹

        è¤‡æ•°ã®ãƒ—ãƒªã‚»ãƒƒãƒˆã‚¯ã‚¨ãƒªï¼ˆrandom influencer, random engineer ãªã©ï¼‰ã‚’
        ãƒ©ãƒ³ãƒ€ãƒ ã«å®Ÿè¡Œã—ã€é‡è¤‡ã‚’é™¤ã„ãŸã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™ã€‚

        Args:
            max_results: å–å¾—ã™ã‚‹æœ€å¤§ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50, ä¸Šé™: 100ï¼‰
            dry_run: True ã®å ´åˆã€ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ï¼ˆGrok API ã‚’å‘¼ã°ãªã„ï¼‰
            category: ã‚«ãƒ†ã‚´ãƒªæŒ‡å®šï¼ˆ'tech', 'business', 'creative', 'science', 'developer', 'product', 'community'ï¼‰

        Returns:
            ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå€™è£œãƒªã‚¹ãƒˆï¼ˆdiscover_accounts_by_keyword ã¨åŒã˜å½¢å¼ï¼‰
        """
        if dry_run:
            logger.info(f"ğŸ­ DRY RUN: ãƒ©ãƒ³ãƒ€ãƒ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
            return self._generate_mock_accounts("random", max_results, "grok_random")

        logger.info(f"ğŸ² Grok Web Search ã§ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’æ¤œç´¢ä¸­...")

        # ãƒ—ãƒªã‚»ãƒƒãƒˆã‚¯ã‚¨ãƒªã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡ï¼ˆå¤šæ§˜æ€§ã‚’ç¢ºä¿ï¼‰
        preset_queries_by_category = {
            'tech': [
                "influential tech Twitter accounts",
                "popular AI researcher on Twitter",
                "famous machine learning engineer on X",
                "influential cybersecurity expert on Twitter",
                "well-known blockchain developer on X",
                "popular cloud architect on Twitter",
                "influential DevOps engineer on X",
                "famous data engineer on Twitter"
            ],
            'business': [
                "famous startup founder on X",
                "influential entrepreneur on X",
                "popular venture capitalist on Twitter",
                "well-known angel investor on X",
                "influential business executive on Twitter",
                "famous CEO on X platform",
                "popular business strategist on Twitter"
            ],
            'creative': [
                "influential designer on X",
                "popular UX designer on Twitter",
                "famous graphic designer on X",
                "well-known creative director on Twitter",
                "influential illustrator on X",
                "popular digital artist on Twitter",
                "famous photographer on X"
            ],
            'science': [
                "well-known data scientist Twitter",
                "influential researcher on X",
                "popular scientist on Twitter",
                "famous physicist on X",
                "influential biologist on Twitter",
                "well-known chemist on X"
            ],
            'developer': [
                "influential developer on Twitter",
                "famous open source contributor on X",
                "popular software engineer on Twitter",
                "influential backend engineer on X",
                "famous frontend developer on Twitter",
                "well-known full stack developer on X"
            ],
            'product': [
                "famous product manager Twitter",
                "influential product strategist on X",
                "popular product designer on Twitter",
                "well-known product marketing on X"
            ],
            'community': [
                "influential tech writer on Twitter",
                "famous tech blogger on X",
                "popular tech podcaster on Twitter",
                "influential tech community leader on X",
                "well-known tech educator on Twitter"
            ]
        }

        # ã‚«ãƒ†ã‚´ãƒªæŒ‡å®šãŒã‚ã‚‹å ´åˆã¯è©²å½“ã‚«ãƒ†ã‚´ãƒªã®ã¿ä½¿ç”¨
        if category and category in preset_queries_by_category:
            preset_queries = preset_queries_by_category[category].copy()
            logger.info(f"ğŸ“‚ ã‚«ãƒ†ã‚´ãƒª '{category}' ã‚’æŒ‡å®š: {len(preset_queries)}ä»¶ã®ã‚¯ã‚¨ãƒª")
        else:
            # å…¨ãƒ—ãƒªã‚»ãƒƒãƒˆã‚¯ã‚¨ãƒªã‚’çµåˆ
            preset_queries = []
            for cat, queries in preset_queries_by_category.items():
                preset_queries.extend(queries)
            if category:
                logger.warning(f"âš ï¸  ä¸æ˜ãªã‚«ãƒ†ã‚´ãƒª '{category}'ã€å…¨ã‚«ãƒ†ã‚´ãƒªã‚’ä½¿ç”¨ã—ã¾ã™")

        all_accounts = []
        seen_handles = set()

        # å„ã‚¯ã‚¨ãƒªã§æ¤œç´¢ï¼ˆé‡è¤‡é™¤å¤–ã—ãªãŒã‚‰ max_results ã«é”ã™ã‚‹ã¾ã§ï¼‰
        import random
        random.shuffle(preset_queries)

        for query in preset_queries:
            if len(all_accounts) >= max_results:
                break

            logger.info(f"  ğŸ“¡ ã‚¯ã‚¨ãƒªå®Ÿè¡Œä¸­: '{query}'")
            accounts = self.discover_accounts_by_keyword(
                query,
                max_results=min(20, max_results - len(all_accounts)),  # ä¸€åº¦ã«æœ€å¤§20ä»¶
                dry_run=False,  # å†…éƒ¨ã§ãƒ¢ãƒƒã‚¯ã¯ä½¿ã‚ãªã„
                x_api_client=x_api_client
            )

            # é‡è¤‡é™¤å¤–ã—ãªãŒã‚‰è¿½åŠ 
            for account in accounts:
                handle = account["handle"]
                if handle not in seen_handles:
                    seen_handles.add(handle)
                    account["source"] = "grok_random"  # ã‚½ãƒ¼ã‚¹ã‚’ä¸Šæ›¸ã
                    all_accounts.append(account)

                    if len(all_accounts) >= max_results:
                        break

        logger.info(f"âœ… ãƒ©ãƒ³ãƒ€ãƒ æ¤œç´¢å®Œäº†: {len(all_accounts)}ä»¶ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå€™è£œã‚’ç™ºè¦‹")
        return all_accounts

    def _generate_mock_accounts(
        self,
        keyword: str,
        count: int,
        source: str
    ) -> List[Dict]:
        """
        ãƒ¢ãƒƒã‚¯ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆãƒ†ã‚¹ãƒˆãƒ»dry-run ç”¨ï¼‰

        Args:
            keyword: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè¡¨ç¤ºåã«åæ˜ ï¼‰
            count: ç”Ÿæˆã™ã‚‹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ•°
            source: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆ"grok_keyword" ã¾ãŸã¯ "grok_random"ï¼‰

        Returns:
            ãƒ¢ãƒƒã‚¯ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        """
        mock_accounts = []

        for i in range(min(count, 20)):  # æœ€å¤§20ä»¶
            mock_accounts.append({
                "handle": f"mock_{keyword.replace(' ', '_')}_{i}",
                "display_name": f"Mock {keyword.title()} {i}",
                "confidence": 0.8 + (i % 3) * 0.05,  # 0.80-0.90
                "profile_url": f"https://x.com/mock_{keyword.replace(' ', '_')}_{i}",
                "description": f"Mock account for testing '{keyword}' discovery",
                "source": source
            })

        logger.info(f"ğŸ­ {len(mock_accounts)}ä»¶ã®ãƒ¢ãƒƒã‚¯ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ç”Ÿæˆ")
        return mock_accounts

    def _filter_accounts_by_quality(
        self,
        accounts: List[Dict],
        min_confidence: float = 0.7
    ) -> List[Dict]:
        """
        ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã§ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

        Args:
            accounts: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå€™è£œãƒªã‚¹ãƒˆ
            min_confidence: æœ€å°ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.7ï¼‰

        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆï¼ˆä¿¡é ¼åº¦ã®é™é †ï¼‰
        """
        filtered = [a for a in accounts if a.get('confidence', 0.0) >= min_confidence]
        sorted_accounts = sorted(filtered, key=lambda x: x.get('confidence', 0.0), reverse=True)
        
        if len(accounts) > len(sorted_accounts):
            logger.info(f"ğŸ” å“è³ªãƒ•ã‚£ãƒ«ã‚¿: {len(accounts)} -> {len(sorted_accounts)}ä»¶ (ä¿¡é ¼åº¦ {min_confidence} ä»¥ä¸Š)")
        
        return sorted_accounts

    def check_account_quality(
        self,
        account: str,
        account_info: Dict,
        thresholds: Dict = None,
        x_api_client=None
    ) -> Dict:
        """
        ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®å“è³ªã‚’è©•ä¾¡ï¼ˆå®Ÿä¸–ç•ŒæŒ‡æ¨™ãƒ™ãƒ¼ã‚¹ï¼‰
        
        X APIãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆfollowers_count, tweet_count, last_tweet_atï¼‰ã‚’ä½¿ç”¨ã—ã¦
        å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
        
        Args:
            account: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå
            account_info: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±è¾æ›¸ï¼ˆhandle, confidence, descriptionãªã©ï¼‰
            thresholds: å“è³ªåŸºæº–ã®è¾æ›¸ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: QUALITY_THRESHOLDSï¼‰
            x_api_client: X APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ç”¨ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            {
                'passed': bool,           # å“è³ªåŸºæº–ã‚’æº€ãŸã—ã¦ã„ã‚‹ã‹
                'score': float,           # å“è³ªã‚¹ã‚³ã‚¢ (0.0-1.0)
                'reasons': List[str],     # è©•ä¾¡ç†ç”±
                'recommendation': str     # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            }
        """
        if thresholds is None:
            thresholds = QUALITY_THRESHOLDS.copy()
        
        logger.info(f"ğŸ“Š ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå“è³ªè©•ä¾¡: @{account}")
        
        passed = True
        reasons = []
        metrics_available = False
        
        # X APIãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆ
        followers_count = None
        tweet_count = None
        last_tweet_at = None
        
        if x_api_client:
            try:
                metrics = x_api_client.fetch_user_metrics(account)
                if metrics:
                    followers_count = metrics.get('followers_count', 0)
                    tweet_count = metrics.get('tweet_count', 0)
                    last_tweet_at = metrics.get('last_tweet_at')
                    metrics_available = True
                    logger.info(
                        f"  X APIãƒ¡ãƒˆãƒªã‚¯ã‚¹: ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼={followers_count}, "
                        f"ãƒ„ã‚¤ãƒ¼ãƒˆ={tweet_count}, æœ€çµ‚æŠ•ç¨¿={last_tweet_at or 'ä¸æ˜'}"
                    )
            except Exception as e:
                logger.warning(f"X APIãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼ï¼ˆç¶šè¡Œï¼‰: {str(e)}")
        
        # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±ã‹ã‚‰ã‚‚å–å¾—ã‚’è©¦è¡Œï¼ˆGrokç™ºè¦‹æ™‚ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰
        if not metrics_available:
            public_metrics = account_info.get('public_metrics', {})
            if public_metrics:
                followers_count = public_metrics.get('followers_count', 0)
                tweet_count = public_metrics.get('tweet_count', 0)
                metrics_available = True
                logger.info(f"  Grokç™ºè¦‹æ™‚ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹: ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼={followers_count}, ãƒ„ã‚¤ãƒ¼ãƒˆ={tweet_count}")
        
        # å®Ÿä¸–ç•ŒæŒ‡æ¨™ãƒ™ãƒ¼ã‚¹ã®å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
        if metrics_available and (followers_count is not None or tweet_count is not None):
            # æ­£è¦åŒ–ã•ã‚ŒãŸã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ï¼‰ã‚’è¨ˆç®—
            # 1. ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ã‚¹ã‚³ã‚¢ï¼ˆ0.5ã®é‡ã¿ï¼‰
            followers_norm = 0.0
            if followers_count is not None:
                # 1000ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã§0.5ã€10000ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã§1.0ã«ãªã‚‹å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«
                if followers_count >= 10000:
                    followers_norm = 1.0
                elif followers_count >= 1000:
                    followers_norm = 0.5 + 0.5 * ((followers_count - 1000) / 9000)
                elif followers_count >= thresholds['min_followers']:
                    followers_norm = 0.3 * (followers_count / thresholds['min_followers'])
                else:
                    followers_norm = 0.1 * (followers_count / thresholds['min_followers'])
                reasons.append(f"ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°: {followers_count} (æ­£è¦åŒ–ã‚¹ã‚³ã‚¢: {followers_norm:.2f})")
            else:
                reasons.append("ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°: ä¸æ˜ï¼ˆã‚¹ã‚³ã‚¢0.0ï¼‰")
            
            # 2. æœ€çµ‚ãƒ„ã‚¤ãƒ¼ãƒˆã®æ–°ã—ã•ã‚¹ã‚³ã‚¢ï¼ˆ0.3ã®é‡ã¿ï¼‰
            recency_norm = 0.0
            if last_tweet_at:
                try:
                    from datetime import datetime, timezone
                    tweet_date = datetime.fromisoformat(last_tweet_at.replace('Z', '+00:00'))
                    days_inactive = (datetime.now(timezone.utc) - tweet_date).days
                    
                    if days_inactive <= 30:
                        recency_norm = 1.0
                    elif days_inactive <= 90:
                        recency_norm = 0.7
                    elif days_inactive <= thresholds['max_days_inactive']:
                        recency_norm = 0.3
                    else:
                        recency_norm = 0.0
                    
                    reasons.append(f"æœ€çµ‚æŠ•ç¨¿: {days_inactive}æ—¥å‰ (æ­£è¦åŒ–ã‚¹ã‚³ã‚¢: {recency_norm:.2f})")
                except Exception as e:
                    logger.warning(f"æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                    reasons.append("æœ€çµ‚æŠ•ç¨¿: æ—¥ä»˜ä¸æ˜ï¼ˆã‚¹ã‚³ã‚¢0.0ï¼‰")
            else:
                reasons.append("æœ€çµ‚æŠ•ç¨¿: ä¸æ˜ï¼ˆã‚¹ã‚³ã‚¢0.0ï¼‰")
            
            # 3. ãƒ„ã‚¤ãƒ¼ãƒˆæ•°ã‚¹ã‚³ã‚¢ï¼ˆ0.2ã®é‡ã¿ï¼‰
            postcount_norm = 0.0
            if tweet_count is not None:
                if tweet_count >= 1000:
                    postcount_norm = 1.0
                elif tweet_count >= thresholds['min_tweet_count']:
                    postcount_norm = 0.5 + 0.5 * ((tweet_count - thresholds['min_tweet_count']) / 950)
                else:
                    postcount_norm = 0.3 * (tweet_count / thresholds['min_tweet_count'])
                reasons.append(f"ãƒ„ã‚¤ãƒ¼ãƒˆæ•°: {tweet_count} (æ­£è¦åŒ–ã‚¹ã‚³ã‚¢: {postcount_norm:.2f})")
            else:
                reasons.append("ãƒ„ã‚¤ãƒ¼ãƒˆæ•°: ä¸æ˜ï¼ˆã‚¹ã‚³ã‚¢0.0ï¼‰")
            
            # åŠ é‡åˆè¨ˆã§å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            score = 0.5 * followers_norm + 0.3 * recency_norm + 0.2 * postcount_norm
            score = max(0.0, min(1.0, score))  # 0.0-1.0ã«åˆ¶é™
            
            # æœ€ä½åŸºæº–ãƒã‚§ãƒƒã‚¯
            if followers_count is not None and followers_count < thresholds['min_followers']:
                passed = False
                reasons.append(f"ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãŒæœ€å°åŸºæº–æœªæº€ ({followers_count} < {thresholds['min_followers']})")
            
            if tweet_count is not None and tweet_count < thresholds['min_tweet_count']:
                passed = False
                reasons.append(f"ãƒ„ã‚¤ãƒ¼ãƒˆæ•°ãŒæœ€å°åŸºæº–æœªæº€ ({tweet_count} < {thresholds['min_tweet_count']})")
            
            if last_tweet_at:
                try:
                    from datetime import datetime, timezone
                    tweet_date = datetime.fromisoformat(last_tweet_at.replace('Z', '+00:00'))
                    days_inactive = (datetime.now(timezone.utc) - tweet_date).days
                    if days_inactive > thresholds['max_days_inactive']:
                        passed = False
                        reasons.append(f"éã‚¢ã‚¯ãƒ†ã‚£ãƒ–æœŸé–“ãŒé•·ã™ãã‚‹ ({days_inactive}æ—¥ > {thresholds['max_days_inactive']}æ—¥)")
                except:
                    pass
            
            if score < thresholds['min_quality_score']:
                passed = False
                reasons.append(f"å“è³ªã‚¹ã‚³ã‚¢ãŒæœ€å°åŸºæº–æœªæº€ ({score:.2f} < {thresholds['min_quality_score']})")
        else:
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå–å¾—ã§ããªã„å ´åˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡
            confidence = account_info.get('confidence', 0.0)
            description = account_info.get('description', '')
            
            if confidence < 0.7:
                passed = False
                score = confidence * 0.8  # ä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹ã®æš«å®šã‚¹ã‚³ã‚¢
            else:
                score = 0.5 + (confidence - 0.5) * 0.5
            
            if not description or len(description.strip()) < 20:
                score *= 0.9
                reasons.append("èª¬æ˜æ–‡ãŒä¸ååˆ†")
            
            reasons.append(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹æœªå–å¾—ï¼ˆä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹è©•ä¾¡: {confidence:.2f}ï¼‰")
            if x_api_client is None:
                reasons.append("X API metrics unavailable â€“ fallback evaluation")
            logger.warning("X APIãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹ã®æš«å®šè©•ä¾¡ã‚’å®Ÿæ–½")
        
        # ãƒãƒ³ãƒ‰ãƒ«ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        handle = account_info.get('handle', '')
        if len(handle) < 3 or len(handle) > 15:
            passed = False
            reasons.append(f"ãƒãƒ³ãƒ‰ãƒ«ãŒä¸è‡ªç„¶ (@{handle})")
        
        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        if passed and score >= 0.7:
            recommendation = "é«˜å“è³ªã‚¢ã‚«ã‚¦ãƒ³ãƒˆ - æ¨å¥¨"
        elif passed:
            recommendation = "å“è³ªåŸºæº–ã‚’æº€ãŸã™ - ä½¿ç”¨å¯èƒ½"
        else:
            recommendation = "å“è³ªåŸºæº–æœªæº€ - é™¤å¤–æ¨å¥¨"
        
        result = {
            'passed': passed,
            'score': score,
            'reasons': reasons,
            'recommendation': recommendation
        }
        
        logger.info(f"  çµæœ: {'âœ…' if passed else 'âŒ'} {score:.2f} - {recommendation}")
        
        return result
