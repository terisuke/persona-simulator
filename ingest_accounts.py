#!/usr/bin/env python3
"""
ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰å‡¦ç† CLI

ä½¿ç”¨ä¾‹:
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¸€æ‹¬å–å¾—
    python ingest_accounts.py accounts.csv --batch-size 5

    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
    python ingest_accounts.py accounts.txt --batch-size 10

    # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å¼·åˆ¶å†å–å¾—
    python ingest_accounts.py accounts.csv --force-refresh

    # Webæ¤œç´¢ã‚’ç„¡åŠ¹åŒ–ã—ã¦é«˜é€ŸåŒ–
    python ingest_accounts.py accounts.csv --no-web-enrichment
"""

import argparse
import logging
import sys
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from enum import Enum

from utils.bootstrap import (
    ensure_cache_dir,
    cache_data,
    load_cache,
    load_grok_api_from_env,
    load_x_api_from_env,
    load_secrets_from_toml,
    read_accounts_from_file,
    DEFAULT_POST_LIMIT
)
from utils.grok_api import GrokAPI
from utils.x_api import XAPIClient

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’äº‹å‰ã«ä½œæˆï¼ˆãƒ­ã‚°åˆæœŸåŒ–å‰ã«å¿…é ˆï¼‰
ensure_cache_dir()

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('.cache/ingest.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


class FetchStatus(Enum):
    """å–å¾—ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    SUCCESS = "success"  # æ–°è¦å–å¾—æˆåŠŸ
    CACHED = "cached"    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨
    FAILED = "failed"    # å¤±æ•—


@dataclass
class FetchResult:
    """å–å¾—çµæœ"""
    posts: List[Dict]
    persona: Dict
    status: FetchStatus
    source: str  # "twitter" | "web_search" | "generated"


class RateLimitManager:
    """X API ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆç®¡ç†"""

    def __init__(self):
        self.remaining_calls = 15  # X API v2 ã®åˆæœŸå€¤
        self.reset_time = None
        self.last_response_headers = {}

    def update_from_headers(self, headers: Dict[str, str]):
        """
        ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæƒ…å ±ã‚’æ›´æ–°

        Args:
            headers: API ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼
        """
        if 'x-rate-limit-remaining' in headers:
            self.remaining_calls = int(headers['x-rate-limit-remaining'])
            logger.debug(f"ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæ®‹ã‚Š: {self.remaining_calls}")

        if 'x-rate-limit-reset' in headers:
            reset_timestamp = int(headers['x-rate-limit-reset'])
            self.reset_time = datetime.fromtimestamp(reset_timestamp)
            logger.debug(f"ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãƒªã‚»ãƒƒãƒˆ: {self.reset_time}")

        self.last_response_headers = headers

    def should_wait(self, threshold: int = 3) -> bool:
        """
        å¾…æ©ŸãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯

        Args:
            threshold: æ®‹ã‚Šå‘¼ã³å‡ºã—æ•°ã®é–¾å€¤

        Returns:
            å¾…æ©ŸãŒå¿…è¦ãªå ´åˆ True
        """
        return self.remaining_calls <= threshold

    def wait_if_needed(self, threshold: int = 3):
        """
        å¿…è¦ã«å¿œã˜ã¦ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãƒªã‚»ãƒƒãƒˆã¾ã§å¾…æ©Ÿ

        Args:
            threshold: æ®‹ã‚Šå‘¼ã³å‡ºã—æ•°ã®é–¾å€¤
        """
        if self.should_wait(threshold):
            if self.reset_time:
                wait_seconds = (self.reset_time - datetime.now()).total_seconds()
                if wait_seconds > 0:
                    logger.warning(
                        f"â³ ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæ¥è¿‘ (æ®‹ã‚Š{self.remaining_calls}å›)ã€‚"
                        f"{int(wait_seconds)}ç§’å¾…æ©Ÿã—ã¾ã™..."
                    )
                    time.sleep(wait_seconds + 5)  # ä½™è£•ã‚’æŒã£ã¦5ç§’è¿½åŠ 
                    self.remaining_calls = 15  # ãƒªã‚»ãƒƒãƒˆ
                    logger.info("âœ… ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãƒªã‚»ãƒƒãƒˆå®Œäº†")
            else:
                # reset_time ãŒä¸æ˜ãªå ´åˆã¯å®‰å…¨ã«15åˆ†å¾…æ©Ÿ
                logger.warning("â³ ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæ¥è¿‘ã€‚å®‰å…¨ã®ãŸã‚15åˆ†å¾…æ©Ÿã—ã¾ã™...")
                time.sleep(900)
                self.remaining_calls = 15

    def decrement(self):
        """å‘¼ã³å‡ºã—å›æ•°ã‚’ãƒ‡ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ"""
        if self.remaining_calls > 0:
            self.remaining_calls -= 1


def fetch_account_data(
    grok_api: GrokAPI,
    x_api: Optional[XAPIClient],
    account: str,
    enable_web_enrichment: bool,
    rate_limiter: RateLimitManager,
    force_refresh: bool = False
) -> FetchResult:
    """
    å˜ä¸€ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

    Args:
        grok_api: Grok API ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        x_api: X API ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
        account: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå
        enable_web_enrichment: Webæ¤œç´¢ã§æƒ…å ±ã‚’å¼·åŒ–ã™ã‚‹ã‹
        rate_limiter: ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        force_refresh: å¼·åˆ¶å†å–å¾—ãƒ•ãƒ©ã‚°

    Returns:
        FetchResult: å–å¾—çµæœï¼ˆposts, persona, statusï¼‰
    """
    account_clean = account.lstrip('@')
    cache_key = f"posts_{account_clean}"

    # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    if not force_refresh:
        cached = load_cache(cache_key)
        if cached:
            logger.info(f"ğŸ“¦ @{account_clean}: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿ (ã‚¹ã‚­ãƒƒãƒ—)")
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã•ã‚ŒãŸsourceã‚’ä½¿ç”¨ï¼ˆãªã‘ã‚Œã°"unknown"ï¼‰
            cached_source = cached.get('source', 'unknown')
            return FetchResult(
                posts=cached['posts'],
                persona=cached['persona'],
                status=FetchStatus.CACHED,
                source=cached_source
            )

    # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãƒã‚§ãƒƒã‚¯
    rate_limiter.wait_if_needed()

    try:
        # æŠ•ç¨¿å–å¾—
        logger.info(f"ğŸ“¡ @{account_clean}: æŠ•ç¨¿å–å¾—ä¸­...")
        posts = grok_api.fetch_posts(
            account_clean,
            limit=DEFAULT_POST_LIMIT,
            since_date="2024-01-01",
            x_api_client=x_api
        )

        # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæ›´æ–° (X API ä½¿ç”¨æ™‚)
        if x_api and hasattr(grok_api, '_last_response_headers'):
            rate_limiter.update_from_headers(grok_api._last_response_headers)
        rate_limiter.decrement()

        if not posts:
            logger.warning(f"âš ï¸  @{account_clean}: æŠ•ç¨¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return FetchResult(posts=[], persona={}, status=FetchStatus.FAILED, source="unknown")

        # å–å¾—æ–¹æ³•ã‚’åˆ¤å®š
        if posts[0]['id'].startswith('web_search_'):
            source = "web_search"
            logger.info(f"âœ… @{account_clean}: {len(posts)}ä»¶å–å¾— (ğŸŒ Grok Web Search)")
        elif posts[0]['id'].startswith('sample_') or posts[0]['id'].startswith('generated_'):
            source = "generated"
            logger.info(f"ğŸ“ @{account_clean}: {len(posts)}ä»¶ç”Ÿæˆ (âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)")
        else:
            source = "twitter"
            logger.info(f"âœ… @{account_clean}: {len(posts)}ä»¶å–å¾— (ğŸ”‘ X API v2)")

        # ãƒšãƒ«ã‚½ãƒŠç”Ÿæˆ
        logger.info(f"ğŸ§  @{account_clean}: ãƒšãƒ«ã‚½ãƒŠç”Ÿæˆä¸­...")
        persona_profile = grok_api.generate_persona_profile(
            posts,
            account=account_clean,
            enable_web_enrichment=enable_web_enrichment
        )

        if persona_profile:
            enrichment_note = "(ãƒãƒ«ãƒãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ )" if enable_web_enrichment else ""
            logger.info(
                f"âœ… @{account_clean}: ãƒšãƒ«ã‚½ãƒŠç”Ÿæˆå®Œäº†{enrichment_note} - "
                f"{persona_profile.get('name', account_clean)}"
            )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        data = {
            'posts': posts,
            'persona': persona_profile,
            'fetched_at': datetime.now().isoformat(),
            'source': source  # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ä¿å­˜
        }
        cache_data(cache_key, data)
        logger.info(f"ğŸ’¾ @{account_clean}: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å®Œäº†")

        return FetchResult(
            posts=posts,
            persona=persona_profile,
            status=FetchStatus.SUCCESS,
            source=source
        )

    except Exception as e:
        logger.error(f"âŒ @{account_clean}: ã‚¨ãƒ©ãƒ¼ - {str(e)}", exc_info=True)
        return FetchResult(posts=[], persona={}, status=FetchStatus.FAILED, source="unknown")


def process_accounts_batch(
    accounts: List[str],
    grok_api: GrokAPI,
    x_api: Optional[XAPIClient],
    batch_size: int,
    enable_web_enrichment: bool,
    force_refresh: bool
) -> Dict[str, any]:
    """
    ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’ãƒãƒƒãƒå‡¦ç†

    Args:
        accounts: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåãƒªã‚¹ãƒˆ
        grok_api: Grok API ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        x_api: X API ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        enable_web_enrichment: Webæ¤œç´¢ã‚’æœ‰åŠ¹åŒ–
        force_refresh: å¼·åˆ¶å†å–å¾—

    Returns:
        å‡¦ç†çµæœã®çµ±è¨ˆæƒ…å ±
    """
    total = len(accounts)
    success_count = 0
    failed_count = 0
    skipped_count = 0

    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
    twitter_count = 0
    web_search_count = 0
    generated_count = 0

    rate_limiter = RateLimitManager()

    logger.info("=" * 80)
    logger.info(f"ğŸš€ ä¸€æ‹¬å‡¦ç†é–‹å§‹: {total}ä»¶ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ")
    logger.info(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    logger.info(f"   Webæ¤œç´¢: {'æœ‰åŠ¹' if enable_web_enrichment else 'ç„¡åŠ¹'}")
    logger.info(f"   å¼·åˆ¶å†å–å¾—: {'æœ‰åŠ¹' if force_refresh else 'ç„¡åŠ¹'}")
    logger.info("=" * 80)

    start_time = time.time()

    for i, account in enumerate(accounts, 1):
        logger.info(f"\n[{i}/{total}] å‡¦ç†ä¸­: @{account}")

        result = fetch_account_data(
            grok_api=grok_api,
            x_api=x_api,
            account=account,
            enable_web_enrichment=enable_web_enrichment,
            rate_limiter=rate_limiter,
            force_refresh=force_refresh
        )

        if result.status == FetchStatus.SUCCESS:
            success_count += 1
        elif result.status == FetchStatus.CACHED:
            skipped_count += 1
        elif result.status == FetchStatus.FAILED:
            failed_count += 1

        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
        if result.source == "twitter":
            twitter_count += 1
        elif result.source == "web_search":
            web_search_count += 1
        elif result.source == "generated":
            generated_count += 1

        # é€²æ—è¡¨ç¤º
        progress_pct = (i / total) * 100
        logger.info(
            f"ğŸ“Š é€²æ—: {i}/{total} ({progress_pct:.1f}%) | "
            f"æˆåŠŸ: {success_count} | ã‚¹ã‚­ãƒƒãƒ—: {skipped_count} | å¤±æ•—: {failed_count}"
        )

        # ãƒãƒƒãƒé–“ã®å¾…æ©Ÿ (X APIè² è·è»½æ¸›)
        if i % batch_size == 0 and i < total:
            wait_time = 2
            logger.info(f"â¸ï¸  ãƒãƒƒãƒ{i // batch_size}å®Œäº†ã€‚{wait_time}ç§’å¾…æ©Ÿ...")
            time.sleep(wait_time)

    elapsed_time = time.time() - start_time

    # çµæœã‚µãƒãƒªãƒ¼
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‰ ä¸€æ‹¬å‡¦ç†å®Œäº†")
    logger.info("=" * 80)
    logger.info(f"ç·ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ•°: {total}")
    logger.info(f"  âœ… æˆåŠŸ: {success_count}")
    logger.info(f"  ğŸ“¦ ã‚¹ã‚­ãƒƒãƒ— (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨): {skipped_count}")
    logger.info(f"  âŒ å¤±æ•—: {failed_count}")
    logger.info("")
    logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å†…è¨³:")
    logger.info(f"  ğŸ”‘ X API (Twitter): {twitter_count}")
    logger.info(f"  ğŸŒ Grok Web Search: {web_search_count}")
    logger.info(f"  ğŸ“ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆ: {generated_count}")

    # å®Ÿãƒ‡ãƒ¼ã‚¿æ¯”ç‡ã‚’è¨ˆç®—
    real_data_count = twitter_count + web_search_count
    if total > 0:
        real_data_ratio = (real_data_count / total) * 100
        logger.info(f"  ğŸ’¡ å®Ÿãƒ‡ãƒ¼ã‚¿æ¯”ç‡: {real_data_ratio:.1f}% ({real_data_count}/{total})")

    logger.info("")
    logger.info(f"å‡¦ç†æ™‚é–“: {elapsed_time:.1f}ç§’ ({elapsed_time / 60:.1f}åˆ†)")
    logger.info("=" * 80)

    return {
        'total': total,
        'success': success_count,
        'skipped': skipped_count,
        'failed': failed_count,
        'twitter': twitter_count,
        'web_search': web_search_count,
        'generated': generated_count,
        'real_data_ratio': (real_data_count / total * 100) if total > 0 else 0,
        'elapsed_time': elapsed_time
    }


def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    parser = argparse.ArgumentParser(
        description='ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰å‡¦ç† CLI',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¸€æ‹¬å–å¾— (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š)
  python ingest_accounts.py accounts.csv

  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æŒ‡å®š
  python ingest_accounts.py accounts.csv --batch-size 10

  # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å¼·åˆ¶å†å–å¾—
  python ingest_accounts.py accounts.csv --force-refresh

  # Webæ¤œç´¢ã‚’ç„¡åŠ¹åŒ–ã—ã¦é«˜é€ŸåŒ–
  python ingest_accounts.py accounts.csv --no-web-enrichment

ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼:
  CSV: account, username, name, handle ã®ã„ãšã‚Œã‹ã®åˆ—ã‚’å«ã‚€
  TXT: 1è¡Œ1ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ (# ã§å§‹ã¾ã‚‹è¡Œã¯ã‚³ãƒ¡ãƒ³ãƒˆ)
        """
    )

    parser.add_argument(
        'accounts_file',
        type=str,
        help='ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« (CSV ã¾ãŸã¯ TXT)'
    )

    parser.add_argument(
        '--batch-size',
        type=int,
        default=5,
        help='ãƒãƒƒãƒã‚µã‚¤ã‚º (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ã€X API ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–)'
    )

    parser.add_argument(
        '--force-refresh',
        action='store_true',
        help='æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å¼·åˆ¶å†å–å¾—'
    )

    parser.add_argument(
        '--no-web-enrichment',
        action='store_true',
        help='Webæ¤œç´¢ã«ã‚ˆã‚‹æƒ…å ±å¼·åŒ–ã‚’ç„¡åŠ¹åŒ– (é«˜é€ŸåŒ–)'
    )

    parser.add_argument(
        '--secrets',
        type=str,
        default='.streamlit/secrets.toml',
        help='secrets.toml ã®ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: .streamlit/secrets.toml)'
    )

    parser.add_argument(
        '--log-level',
        type=str,
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        help='ãƒ­ã‚°ãƒ¬ãƒ™ãƒ« (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: INFO)'
    )

    args = parser.parse_args()

    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š
    logging.getLogger().setLevel(getattr(logging, args.log_level))

    # secrets.toml ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
    logger.info(f"ğŸ“– è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {args.secrets}")
    secrets = load_secrets_from_toml(args.secrets)
    if not secrets:
        logger.error("âŒ secrets.toml ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    # API åˆæœŸåŒ–
    logger.info("ğŸ”§ APIåˆæœŸåŒ–ä¸­...")
    grok_api = load_grok_api_from_env()
    if not grok_api:
        logger.error("âŒ Grok API ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚GROK_API_KEY ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    x_api = load_x_api_from_env()
    if x_api:
        logger.info("âœ… X API v2 ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
    else:
        logger.info("â„¹ï¸  X API v2 ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ (Grok Web Search ã‚’ä½¿ç”¨)")

    # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿
    logger.info(f"ğŸ“‹ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿: {args.accounts_file}")
    accounts = read_accounts_from_file(args.accounts_file)

    if not accounts:
        logger.error("âŒ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    logger.info(f"âœ… {len(accounts)} ä»¶ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
    results = process_accounts_batch(
        accounts=accounts,
        grok_api=grok_api,
        x_api=x_api,
        batch_size=args.batch_size,
        enable_web_enrichment=not args.no_web_enrichment,
        force_refresh=args.force_refresh
    )

    # çµ‚äº†ã‚³ãƒ¼ãƒ‰
    if results['failed'] > 0:
        logger.warning(f"âš ï¸  {results['failed']} ä»¶ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§å‡¦ç†ãŒå¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
    else:
        logger.info("âœ… ã™ã¹ã¦ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        sys.exit(0)


if __name__ == "__main__":
    main()
