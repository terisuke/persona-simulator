#!/usr/bin/env python3
"""
ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰å‡¦ç† CLI

ä½¿ç”¨ä¾‹:
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¸€æ‹¬å–å¾—
    python ingest_accounts.py accounts.csv --batch-size 5

    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
    python ingest_accounts.py accounts.txt --batch-size 10

    # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å¼·åˆ¶å†å–å¾—
    python ingest_accounts.py accounts.csv --force-refresh

    # Webæ¤œç´¢ã‚’ç„¡åŠ¹åŒ–ã—ã¦é«˜é€ŸåŒ–
    python ingest_accounts.py accounts.csv --no-web-enrichment
"""

import argparse
import os
import logging
import sys
import time
from collections import Counter
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from enum import Enum

from utils.bootstrap import (
    ensure_cache_dir,
    cache_data,
    load_cache,
    load_grok_api_from_env,
    load_x_api_from_env,
    load_secrets_from_toml,
    read_accounts_from_file,
    DEFAULT_POST_LIMIT
)
from utils.grok_api import GrokAPI, PRESET_KEYWORDS
from utils.x_api import XAPIClient

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’äº‹å‰ã«ä½œæˆï¼ˆãƒ­ã‚°åˆæœŸåŒ–å‰ã«å¿…é ˆï¼‰
ensure_cache_dir()

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('.cache/ingest.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ç”Ÿæˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®è¨±å¯ãƒ•ãƒ©ã‚°ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ç½®ï¼‰ã€‚
# ç¾åœ¨ã¯å¸¸ã« Falseï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã®ã¿é‹ç”¨ï¼‰ã€‚
ALLOW_GENERATED_FLAG: bool = False


def delete_cache_file(cache_key: str):
    """å¯¾è±¡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
    cache_path = os.path.join('.cache', f"{cache_key}.pkl")
    if os.path.exists(cache_path):
        try:
            os.remove(cache_path)
            logger.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤: {cache_key}")
        except Exception as error:
            logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤å¤±æ•—({cache_key}): {error}")


class FetchStatus(Enum):
    """å–å¾—ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    SUCCESS = "success"  # æ–°è¦å–å¾—æˆåŠŸ
    CACHED = "cached"    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨
    FAILED = "failed"    # å¤±æ•—


@dataclass
class FetchResult:
    """å–å¾—çµæœ

    Stage3 å¤š SNS é€£æºã«å‘ã‘ãŸæ‹¡å¼µãƒã‚¤ãƒ³ãƒˆ:
    - source ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ä»¥ä¸‹ã®å€¤ã‚’ã‚µãƒãƒ¼ãƒˆäºˆå®š:
      - "twitter": X API v2 çµŒç”±ã§å–å¾—
      - "web_search": Grok Web Search çµŒç”±ã§å–å¾—
      - "facebook": Facebook Graph API çµŒç”±ï¼ˆStage3ï¼‰
      - "instagram": Instagram Graph API çµŒç”±ï¼ˆStage3ï¼‰
      - "linkedin": LinkedIn Marketing API çµŒç”±ï¼ˆStage3ï¼‰
      - "tiktok": TikTok Research API çµŒç”±ï¼ˆStage3ï¼‰
    - å°†æ¥çš„ã«ã¯è¤‡æ•°ã‚½ãƒ¼ã‚¹ã®çµ±åˆï¼ˆä¾‹: "twitter,linkedin"ï¼‰ã‚‚æ¤œè¨
    """
    posts: List[Dict]
    persona: Dict
    status: FetchStatus
    source: str  # "twitter" | "web_search" | "facebook" | "instagram" | "linkedin" | "tiktok"


class RateLimitManager:
    """X API ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆç®¡ç†"""

    def __init__(self):
        self.remaining_calls = 15  # X API v2 ã®åˆæœŸå€¤
        self.reset_time = None
        self.last_response_headers = {}

    def update_from_headers(self, headers: Dict[str, str]):
        """
        ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæƒ…å ±ã‚’æ›´æ–°

        Args:
            headers: API ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼
        """
        if 'x-rate-limit-remaining' in headers:
            self.remaining_calls = int(headers['x-rate-limit-remaining'])
            logger.debug(f"ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæ®‹ã‚Š: {self.remaining_calls}")

        if 'x-rate-limit-reset' in headers:
            reset_timestamp = int(headers['x-rate-limit-reset'])
            self.reset_time = datetime.fromtimestamp(reset_timestamp)
            logger.debug(f"ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãƒªã‚»ãƒƒãƒˆ: {self.reset_time}")

        self.last_response_headers = headers

    def should_wait(self, threshold: int = 3) -> bool:
        """
        å¾…æ©ŸãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯

        Args:
            threshold: æ®‹ã‚Šå‘¼ã³å‡ºã—æ•°ã®é–¾å€¤

        Returns:
            å¾…æ©ŸãŒå¿…è¦ãªå ´åˆ True
        """
        return self.remaining_calls <= threshold

    def wait_if_needed(self, threshold: int = 3):
        """
        å¿…è¦ã«å¿œã˜ã¦ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãƒªã‚»ãƒƒãƒˆã¾ã§å¾…æ©Ÿ

        Args:
            threshold: æ®‹ã‚Šå‘¼ã³å‡ºã—æ•°ã®é–¾å€¤
        """
        if self.should_wait(threshold):
            if self.reset_time:
                wait_seconds = (self.reset_time - datetime.now()).total_seconds()
                if wait_seconds > 0:
                    logger.warning(
                        f"â³ ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæ¥è¿‘ (æ®‹ã‚Š{self.remaining_calls}å›)ã€‚"
                        f"{int(wait_seconds)}ç§’å¾…æ©Ÿã—ã¾ã™..."
                    )
                    time.sleep(wait_seconds + 5)  # ä½™è£•ã‚’æŒã£ã¦5ç§’è¿½åŠ 
                    self.remaining_calls = 15  # ãƒªã‚»ãƒƒãƒˆ
                    logger.info("âœ… ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãƒªã‚»ãƒƒãƒˆå®Œäº†")
            else:
                # reset_time ãŒä¸æ˜ãªå ´åˆã¯å®‰å…¨ã«15åˆ†å¾…æ©Ÿ
                logger.warning("â³ ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæ¥è¿‘ã€‚å®‰å…¨ã®ãŸã‚15åˆ†å¾…æ©Ÿã—ã¾ã™...")
                time.sleep(900)
                self.remaining_calls = 15

    def decrement(self):
        """å‘¼ã³å‡ºã—å›æ•°ã‚’ãƒ‡ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ"""
        if self.remaining_calls > 0:
            self.remaining_calls -= 1


def fetch_account_data(
    grok_api: GrokAPI,
    x_api: Optional[XAPIClient],
    account: str,
    enable_web_enrichment: bool,
    rate_limiter: RateLimitManager,
    force_refresh: bool = False
) -> FetchResult:
    """
    å˜ä¸€ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

    Args:
        grok_api: Grok API ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        x_api: X API ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
        account: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå
        enable_web_enrichment: Webæ¤œç´¢ã§æƒ…å ±ã‚’å¼·åŒ–ã™ã‚‹ã‹
        rate_limiter: ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        force_refresh: å¼·åˆ¶å†å–å¾—ãƒ•ãƒ©ã‚°

    Returns:
        FetchResult: å–å¾—çµæœï¼ˆposts, persona, statusï¼‰
    """
    account_clean = account.lstrip('@')
    cache_key = f"posts_{account_clean}"

    # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    if not force_refresh:
        cached = load_cache(cache_key)
        if cached:
            cached_posts = cached.get('posts', [])
            if cached_posts and (cached_posts[0].get('id', '').startswith('sample_') or cached_posts[0].get('id', '').startswith('generated_')):
                logger.warning(
                    f"âš ï¸ @{account_clean}: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚å‰Šé™¤ã—å†å–å¾—ã—ã¾ã™ã€‚"
                )
                delete_cache_file(cache_key)
            else:
                logger.info(f"ğŸ“¦ @{account_clean}: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿ (ã‚¹ã‚­ãƒƒãƒ—)")
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã•ã‚ŒãŸsourceã‚’ä½¿ç”¨ï¼ˆãªã‘ã‚Œã°"unknown"ï¼‰
                cached_source = cached.get('source', 'unknown')
                return FetchResult(
                    posts=cached['posts'],
                    persona=cached['persona'],
                    status=FetchStatus.CACHED,
                    source=cached_source
                )

    # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãƒã‚§ãƒƒã‚¯
    rate_limiter.wait_if_needed()

    try:
        # æŠ•ç¨¿å–å¾—
        logger.info(f"ğŸ“¡ @{account_clean}: æŠ•ç¨¿å–å¾—ä¸­...")
        posts = grok_api.fetch_posts(
            account_clean,
            limit=DEFAULT_POST_LIMIT,
            since_date="2024-01-01",
            x_api_client=x_api,
            allow_generated=ALLOW_GENERATED_FLAG
        )

        # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæ›´æ–° (X API ä½¿ç”¨æ™‚)
        if x_api and hasattr(grok_api, '_last_response_headers'):
            rate_limiter.update_from_headers(grok_api._last_response_headers)
        rate_limiter.decrement()

        if not posts:
            logger.warning(f"âš ï¸  @{account_clean}: æŠ•ç¨¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return FetchResult(posts=[], persona={}, status=FetchStatus.FAILED, source="unknown")

        # å–å¾—æ–¹æ³•ã‚’åˆ¤å®š
        if posts[0]['id'].startswith('web_search_'):
            source = "web_search"
            logger.info(f"âœ… @{account_clean}: {len(posts)}ä»¶å–å¾— (ğŸŒ Grok Web Search)")
        else:
            source = "twitter"
            logger.info(f"âœ… @{account_clean}: {len(posts)}ä»¶å–å¾— (ğŸ”‘ X API v2)")

        # ãƒšãƒ«ã‚½ãƒŠç”Ÿæˆ
        logger.info(f"ğŸ§  @{account_clean}: ãƒšãƒ«ã‚½ãƒŠç”Ÿæˆä¸­...")
        persona_profile = grok_api.generate_persona_profile(
            posts,
            account=account_clean,
            enable_web_enrichment=enable_web_enrichment
        )

        if persona_profile:
            enrichment_note = "(ãƒãƒ«ãƒãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ )" if enable_web_enrichment else ""
            logger.info(
                f"âœ… @{account_clean}: ãƒšãƒ«ã‚½ãƒŠç”Ÿæˆå®Œäº†{enrichment_note} - "
                f"{persona_profile.get('name', account_clean)}"
            )
            
            # quality_scoreã‚’è©•ä¾¡ã—ã¦ãƒšãƒ«ã‚½ãƒŠã«ä»˜ä¸
            account_info = {
                "handle": account_clean,
                "description": persona_profile.get('background', ''),
                "confidence": 0.8  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆç™ºè¦‹æ™‚ã«ã¯æ—¢ã«è©•ä¾¡æ¸ˆã¿ã®å ´åˆã‚‚ã‚ã‚‹ï¼‰
            }
            quality_result = grok_api.check_account_quality(
                account_clean,
                account_info,
                x_api_client=x_api
            )
            if quality_result:
                persona_profile['quality_score'] = quality_result['score']
                persona_profile['quality_reasons'] = quality_result.get('reasons', [])
                logger.info(f"ğŸ“Š @{account_clean}: quality_score={quality_result['score']:.2f}ã‚’ä»˜ä¸")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        data = {
            'posts': posts,
            'persona': persona_profile,
            'fetched_at': datetime.now().isoformat(),
            'source': source  # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ä¿å­˜
        }
        cache_data(cache_key, data)
        logger.info(f"ğŸ’¾ @{account_clean}: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å®Œäº†")

        return FetchResult(
            posts=posts,
            persona=persona_profile,
            status=FetchStatus.SUCCESS,
            source=source
        )

    except Exception as e:
        logger.error(f"âŒ @{account_clean}: ã‚¨ãƒ©ãƒ¼ - {str(e)}", exc_info=True)
        return FetchResult(posts=[], persona={}, status=FetchStatus.FAILED, source="unknown")


def discover_and_save_accounts(
    grok_api: GrokAPI,
    keyword: Optional[str],
    random: bool,
    max_results: int,
    dry_run: bool,
    category: Optional[str] = None,
    preset: Optional[str] = None,
    x_api: Optional[XAPIClient] = None,
    diversity_sampling: bool = False,
    sampling_method: str = "stratified",
    prefer_x_api: bool = True,
    fallback_to_grok: bool = True,
    quotas: Optional[Dict] = None
) -> Optional[str]:
    """
    Grok Web Search ã§ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå€™è£œã‚’ç™ºè¦‹ã—ã€CSV/TXT ã«ä¿å­˜

    Args:
        grok_api: Grok API ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        keyword: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (keyword ãƒ¢ãƒ¼ãƒ‰æ™‚)
        random: ãƒ©ãƒ³ãƒ€ãƒ ãƒ¢ãƒ¼ãƒ‰
        max_results: æœ€å¤§å–å¾—ä»¶æ•°
        dry_run: ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        category: ã‚«ãƒ†ã‚´ãƒªæŒ‡å®šï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ¢ãƒ¼ãƒ‰æ™‚ï¼‰
        preset: ãƒ—ãƒªã‚»ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŒ‡å®š

    Returns:
        ä¿å­˜ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆå¤±æ•—æ™‚ã¯ Noneï¼‰
    """
    from datetime import datetime
    import os
    import csv

    # .cache/discover_results ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆUI ã¨å…±é€šï¼‰
    discover_dir = os.path.join(".cache", "discover_results")
    if not os.path.exists(discover_dir):
        os.makedirs(discover_dir)
        logger.info(f"ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {discover_dir}")

    diversity_metrics: Dict[str, float] = {}
    diversity_report_path: Optional[str] = None

    # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç™ºè¦‹
    if diversity_sampling:
        logger.info(
            "ğŸ² å¤šæ§˜æ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰) ã‚’é–‹å§‹ "
            f"(sampling_method={sampling_method}, prefer_x_api={prefer_x_api}, "
            f"fallback_to_grok={fallback_to_grok})"
        )
        accounts = grok_api.discover_accounts_with_diversity_hybrid(
            max_results=max_results,
            sampling_method=sampling_method,
            x_api_client=x_api,
            quotas=quotas,
            prefer_x_api=prefer_x_api,
            fallback_to_grok=fallback_to_grok
        )
        mode = "diversity_hybrid"
        filename_base = f"diversity_{sampling_method}_hybrid_accounts"
    elif preset:
        # ãƒ—ãƒªã‚»ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
        actual_keyword = PRESET_KEYWORDS[preset]
        logger.info(f"ğŸ” ãƒ—ãƒªã‚»ãƒƒãƒˆ '{preset}' ({actual_keyword}) ã§ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç™ºè¦‹ä¸­...")
        accounts = grok_api.discover_accounts_by_keyword(
            preset,  # ãƒ—ãƒªã‚»ãƒƒãƒˆåã‚’æ¸¡ã™
            max_results=max_results,
            dry_run=dry_run,
            x_api_client=x_api
        )
        mode = "preset"
        filename_base = f"preset_{preset}"
    elif keyword:
        logger.info(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{keyword}' ã§ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç™ºè¦‹ä¸­...")
        accounts = grok_api.discover_accounts_by_keyword(
            keyword,
            max_results=max_results,
            dry_run=dry_run,
            x_api_client=x_api
        )
        mode = "keyword"
        filename_base = f"keyword_{keyword.replace(' ', '_')}"
    else:  # random
        logger.info(f"ğŸ² ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç™ºè¦‹ä¸­...")
        accounts = grok_api.discover_accounts_random(
            max_results=max_results,
            dry_run=dry_run,
            category=category,
            x_api_client=x_api
        )
        mode = "random"
        filename_base = "random_accounts"
        if category:
            filename_base = f"random_{category}_accounts"

    if not accounts:
        logger.error("âŒ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None

    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_path = os.path.join(discover_dir, f"{filename_base}_{timestamp}.csv")
    txt_path = os.path.join(discover_dir, f"{filename_base}_{timestamp}.txt")

    # CSV ä¿å­˜ï¼ˆquality_score ã‚‚å«ã‚ã‚‹ï¼‰
    fieldnames = [
        'handle',
        'display_name',
        'confidence',
        'profile_url',
        'description',
        'source',
        'quality_score',
        'diversity_score',
        'followers_count',
        'tweet_count',
        'region',
        'language',
        'dominant_sentiment',
        'last_tweet_at',
        'account_created_at'
    ]
    with open(csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
        writer.writeheader()
        for account in accounts:
            writer.writerow(account)

    logger.info(f"ğŸ’¾ CSV ä¿å­˜: {csv_path} ({len(accounts)}ä»¶)")

    # TXT ä¿å­˜ï¼ˆhandle ã®ã¿ã€1è¡Œ1ã‚¢ã‚«ã‚¦ãƒ³ãƒˆï¼‰
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(f"# Discovered accounts via Grok {mode} search\n")
        f.write(f"# Generated: {datetime.now().isoformat()}\n")
        f.write(f"# Total: {len(accounts)} accounts\n")
        f.write("#\n")
        for account in accounts:
            f.write(f"{account['handle']}\n")

    logger.info(f"ğŸ’¾ TXT ä¿å­˜: {txt_path} ({len(accounts)}ä»¶)")

    # å¤šæ§˜æ€§ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
    if diversity_sampling:
        from utils.diversity_sampling import DiversitySampler

        sampler = DiversitySampler(x_api_client=x_api, grok_api=grok_api)
        diversity_metrics = sampler.calculate_diversity_metrics(
            accounts,
            attributes=['followers', 'region', 'language', 'sentiment']
        )
        diversity_report_path = csv_path.replace('.csv', '_diversity_report.txt')

        with open(diversity_report_path, 'w', encoding='utf-8') as f:
            f.write("=== ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å¤šæ§˜æ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆ ===\n\n")
            f.write(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•: {sampling_method}\n")
            f.write(f"X APIå„ªå…ˆ: {prefer_x_api}\n")
            f.write(f"Grokãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {fallback_to_grok}\n")
            f.write(f"ç·ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ•°: {len(accounts)}\n\n")
            f.write("å¤šæ§˜æ€§æŒ‡æ¨™:\n")
            for key, value in diversity_metrics.items():
                f.write(f"  {key}: {value:.3f}\n")
            f.write("\nãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ†å¸ƒ:\n")
            source_counts = Counter(acc.get('source', 'unknown') for acc in accounts)
            for source, count in source_counts.items():
                f.write(f"  {source}: {count}ä»¶\n")

        logger.info(f"ğŸ“Š å¤šæ§˜æ€§ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {diversity_report_path}")

    # çµ±è¨ˆè¡¨ç¤º
    logger.info("")
    logger.info("=" * 80)
    logger.info("ğŸ“Š ç™ºè¦‹çµæœã‚µãƒãƒªãƒ¼")
    logger.info("=" * 80)
    logger.info(f"ãƒ¢ãƒ¼ãƒ‰: {mode}")
    logger.info(f"ç™ºè¦‹ä»¶æ•°: {len(accounts)}")
    if any(a.get('confidence') is not None for a in accounts):
        avg_conf = sum(a.get('confidence', 0.0) for a in accounts) / len(accounts)
        logger.info(f"å¹³å‡ä¿¡é ¼åº¦: {avg_conf:.2f}")
    if diversity_metrics:
        logger.info("å¤šæ§˜æ€§æŒ‡æ¨™:")
        for key, value in diversity_metrics.items():
            logger.info(f"  {key}: {value:.3f}")
    source_distribution = Counter(acc.get('source', 'unknown') for acc in accounts)
    if source_distribution:
        logger.info("ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å†…è¨³:")
        for source, count in source_distribution.items():
            logger.info(f"  {source}: {count}ä»¶")
    logger.info(f"CSV: {csv_path}")
    logger.info(f"TXT: {txt_path}")
    if diversity_report_path:
        logger.info(f"Diversity Report: {diversity_report_path}")
    logger.info("=" * 80)
    logger.info("")
    logger.info("ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    logger.info(f"  python ingest_accounts.py {txt_path}")
    logger.info("=" * 80)

    return csv_path


def process_accounts_batch(
    accounts: List[str],
    grok_api: GrokAPI,
    x_api: Optional[XAPIClient],
    batch_size: int,
    enable_web_enrichment: bool,
    force_refresh: bool
) -> Dict[str, any]:
    """
    ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’ãƒãƒƒãƒå‡¦ç†

    Args:
        accounts: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåãƒªã‚¹ãƒˆ
        grok_api: Grok API ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        x_api: X API ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        enable_web_enrichment: Webæ¤œç´¢ã‚’æœ‰åŠ¹åŒ–
        force_refresh: å¼·åˆ¶å†å–å¾—

    Returns:
        å‡¦ç†çµæœã®çµ±è¨ˆæƒ…å ±
    """
    total = len(accounts)
    success_count = 0
    failed_count = 0
    skipped_count = 0

    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
    twitter_count = 0
    web_search_count = 0
    generated_count = 0
    
    # æœªç¢ºå®šæ•°ã¨quality_scoreã®é›†è¨ˆ
    unverified_count = 0
    quality_scores = []

    rate_limiter = RateLimitManager()

    logger.info("=" * 80)
    logger.info(f"ğŸš€ ä¸€æ‹¬å‡¦ç†é–‹å§‹: {total}ä»¶ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ")
    logger.info(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    logger.info(f"   Webæ¤œç´¢: {'æœ‰åŠ¹' if enable_web_enrichment else 'ç„¡åŠ¹'}")
    logger.info(f"   å¼·åˆ¶å†å–å¾—: {'æœ‰åŠ¹' if force_refresh else 'ç„¡åŠ¹'}")
    logger.info(f"   X APIä½¿ç”¨: {x_api is not None}")
    logger.info("=" * 80)

    start_time = time.time()

    for i, account in enumerate(accounts, 1):
        logger.info(f"\n[{i}/{total}] å‡¦ç†ä¸­: @{account}")

        result = fetch_account_data(
            grok_api=grok_api,
            x_api=x_api,
            account=account,
            enable_web_enrichment=enable_web_enrichment,
            rate_limiter=rate_limiter,
            force_refresh=force_refresh
        )

        if result.status == FetchStatus.SUCCESS:
            success_count += 1
        elif result.status == FetchStatus.CACHED:
            skipped_count += 1
        elif result.status == FetchStatus.FAILED:
            failed_count += 1

        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
        if result.source == "twitter":
            twitter_count += 1
        elif result.source == "web_search":
            web_search_count += 1
        elif result.source == "generated":
            generated_count += 1
        
        # æœªç¢ºå®šæ•°ï¼ˆãƒšãƒ«ã‚½ãƒŠãŒNoneã¾ãŸã¯ç©ºï¼‰
        if not result.persona or len(result.persona) == 0:
            unverified_count += 1
        
        # quality_scoreãŒã‚ã‚Œã°é›†è¨ˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã‚“ã å ´åˆã‚‚å«ã‚€ï¼‰
        if result.persona and 'quality_score' in result.persona:
            quality_scores.append(result.persona['quality_score'])

        # é€²æ—è¡¨ç¤º
        progress_pct = (i / total) * 100
        logger.info(
            f"ğŸ“Š é€²æ—: {i}/{total} ({progress_pct:.1f}%) | "
            f"æˆåŠŸ: {success_count} | ã‚¹ã‚­ãƒƒãƒ—: {skipped_count} | å¤±æ•—: {failed_count}"
        )

        # ãƒãƒƒãƒé–“ã®å¾…æ©Ÿ (X APIè² è·è»½æ¸›)
        if i % batch_size == 0 and i < total:
            wait_time = 2
            logger.info(f"â¸ï¸  ãƒãƒƒãƒ{i // batch_size}å®Œäº†ã€‚{wait_time}ç§’å¾…æ©Ÿ...")
            time.sleep(wait_time)

    elapsed_time = time.time() - start_time

    # çµæœã‚µãƒãƒªãƒ¼
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‰ ä¸€æ‹¬å‡¦ç†å®Œäº†")
    logger.info("=" * 80)
    logger.info(f"ç·ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ•°: {total}")
    logger.info(f"  âœ… æˆåŠŸ: {success_count}")
    logger.info(f"  ğŸ“¦ ã‚¹ã‚­ãƒƒãƒ— (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨): {skipped_count}")
    logger.info(f"  âŒ å¤±æ•—: {failed_count}")
    logger.info("")
    logger.info(f"X APIä½¿ç”¨: {x_api is not None}")
    logger.info("")
    logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å†…è¨³:")
    logger.info(f"  ğŸ”‘ X API (Twitter): {twitter_count}")
    logger.info(f"  ğŸŒ Grok Web Search: {web_search_count}")
    logger.info(f"  ğŸ“ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆ: {generated_count}")

    # å®Ÿãƒ‡ãƒ¼ã‚¿æ¯”ç‡ã¨ç”Ÿæˆãƒ‡ãƒ¼ã‚¿æ¯”ç‡ã‚’è¨ˆç®—
    real_data_count = twitter_count + web_search_count
    if total > 0:
        real_data_ratio = (real_data_count / total) * 100
        generated_data_ratio = (generated_count / total) * 100
        logger.info(f"  ğŸ’¡ å®Ÿãƒ‡ãƒ¼ã‚¿æ¯”ç‡: {real_data_ratio:.1f}% ({real_data_count}/{total})")
        logger.info(f"  âš ï¸  ç”Ÿæˆãƒ‡ãƒ¼ã‚¿æ¯”ç‡: {generated_data_ratio:.1f}% ({generated_count}/{total})")
    
    # æœªç¢ºå®šæ•°
    logger.info("")
    logger.info(f"ğŸ“Š å“è³ªæŒ‡æ¨™:")
    logger.info(f"  âš ï¸  æœªç¢ºå®šãƒšãƒ«ã‚½ãƒŠ: {unverified_count}ä»¶")
    
    # å¹³å‡quality_score
    if quality_scores:
        avg_quality = sum(quality_scores) / len(quality_scores)
        median_quality = sorted(quality_scores)[len(quality_scores) // 2] if quality_scores else 0.0
        logger.info(f"  ğŸ“ˆ å¹³å‡quality_score: {avg_quality:.2f}")
        logger.info(f"  ğŸ“Š ä¸­å¤®å€¤quality_score: {median_quality:.2f} (å¯¾è±¡: {len(quality_scores)}ä»¶)")

    logger.info("")
    logger.info(f"å‡¦ç†æ™‚é–“: {elapsed_time:.1f}ç§’ ({elapsed_time / 60:.1f}åˆ†)")
    logger.info("=" * 80)

    generated_data_ratio = (generated_count / total * 100) if total > 0 else 0
    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else None
    median_quality = sorted(quality_scores)[len(quality_scores) // 2] if quality_scores else None
    
    return {
        'total': total,
        'success': success_count,
        'skipped': skipped_count,
        'failed': failed_count,
        'twitter': twitter_count,
        'web_search': web_search_count,
        'generated': generated_count,
        'real_data_ratio': (real_data_count / total * 100) if total > 0 else 0,
        'generated_data_ratio': generated_data_ratio,
        'unverified_count': unverified_count,
        'avg_quality_score': avg_quality,
        'median_quality_score': median_quality,
        'quality_score_count': len(quality_scores),
        'elapsed_time': elapsed_time
    }


def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    parser = argparse.ArgumentParser(
        description='ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰å‡¦ç† CLI',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¸€æ‹¬å–å¾— (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š)
  python ingest_accounts.py accounts.csv

  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æŒ‡å®š
  python ingest_accounts.py accounts.csv --batch-size 10

  # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å¼·åˆ¶å†å–å¾—
  python ingest_accounts.py accounts.csv --force-refresh

  # Webæ¤œç´¢ã‚’ç„¡åŠ¹åŒ–ã—ã¦é«˜é€ŸåŒ–
  python ingest_accounts.py accounts.csv --no-web-enrichment

ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼:
  CSV: account, username, name, handle ã®ã„ãšã‚Œã‹ã®åˆ—ã‚’å«ã‚€
  TXT: 1è¡Œ1ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ (# ã§å§‹ã¾ã‚‹è¡Œã¯ã‚³ãƒ¡ãƒ³ãƒˆ)
        """
    )

    parser.add_argument(
        'accounts_file',
        type=str,
        nargs='?',  # optionalï¼ˆdiscover ãƒ¢ãƒ¼ãƒ‰æ™‚ã¯ä¸è¦ï¼‰
        help='ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« (CSV ã¾ãŸã¯ TXT)'
    )

    # Stage 2.5: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç™ºè¦‹æ©Ÿèƒ½
    discover_group = parser.add_argument_group('ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç™ºè¦‹ï¼ˆStage 2.5ï¼‰')
    discover_group.add_argument(
        '--discover-keyword',
        type=str,
        metavar='KEYWORD',
        help='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå€™è£œã‚’ç™ºè¦‹ï¼ˆä¾‹: "AI engineer"ï¼‰'
    )
    discover_group.add_argument(
        '--discover-random',
        action='store_true',
        help='ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå€™è£œã‚’ç™ºè¦‹ï¼ˆè¤‡æ•°ãƒ—ãƒªã‚»ãƒƒãƒˆã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œï¼‰'
    )
    discover_group.add_argument(
        '--max-results',
        type=int,
        default=50,
        help='ç™ºè¦‹ã™ã‚‹æœ€å¤§ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50, ä¸Šé™: 100ï¼‰'
    )
    discover_group.add_argument(
        '--dry-run',
        action='store_true',
        help='ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆGrok API ã‚’å‘¼ã°ãªã„ã€ãƒ†ã‚¹ãƒˆç”¨ï¼‰'
    )
    discover_group.add_argument(
        '--category',
        type=str,
        choices=['tech', 'business', 'creative', 'science', 'developer', 'product', 'community'],
        help='ã‚«ãƒ†ã‚´ãƒªæŒ‡å®šï¼ˆãƒ©ãƒ³ãƒ€ãƒ æ¤œç´¢æ™‚ï¼‰- tech, business, creative, science, developer, product, community'
    )
    discover_group.add_argument(
        '--preset',
        type=str,
        choices=list(PRESET_KEYWORDS.keys()),
        help=f'ãƒ—ãƒªã‚»ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŒ‡å®š - {", ".join(sorted(PRESET_KEYWORDS.keys()))}'
    )
    discover_group.add_argument(
        '--diversity-sampling',
        action='store_true',
        help='å¤šæ§˜æ€§ã‚’æ‹…ä¿ã—ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå€™è£œã‚’ç™ºè¦‹'
    )
    discover_group.add_argument(
        '--sampling-method',
        type=str,
        choices=['stratified', 'quota', 'random'],
        default='stratified',
        help='å¤šæ§˜æ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ã®æ‰‹æ³•ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: stratifiedï¼‰'
    )
    discover_group.add_argument(
        '--prefer-x-api',
        dest='prefer_x_api',
        action='store_true',
        help='X APIã‚’å„ªå…ˆã—ã¦ä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æœ‰åŠ¹ï¼‰'
    )
    discover_group.add_argument(
        '--no-prefer-x-api',
        dest='prefer_x_api',
        action='store_false',
        help='X APIå„ªå…ˆã‚’ç„¡åŠ¹åŒ–'
    )
    discover_group.add_argument(
        '--no-fallback-grok',
        action='store_true',
        help='Grok Web Searchã¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ç„¡åŠ¹åŒ–'
    )

    parser.set_defaults(prefer_x_api=True)

    parser.add_argument(
        '--batch-size',
        type=int,
        default=5,
        help='ãƒãƒƒãƒã‚µã‚¤ã‚º (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ã€X API ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–)'
    )

    parser.add_argument(
        '--force-refresh',
        action='store_true',
        help='æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å¼·åˆ¶å†å–å¾—'
    )

    parser.add_argument(
        '--no-web-enrichment',
        action='store_true',
        help='Webæ¤œç´¢ã«ã‚ˆã‚‹æƒ…å ±å¼·åŒ–ã‚’ç„¡åŠ¹åŒ– (é«˜é€ŸåŒ–)'
    )

    parser.add_argument(
        '--secrets',
        type=str,
        default='.streamlit/secrets.toml',
        help='secrets.toml ã®ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: .streamlit/secrets.toml)'
    )

    parser.add_argument(
        '--log-level',
        type=str,
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        help='ãƒ­ã‚°ãƒ¬ãƒ™ãƒ« (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: INFO)'
    )

    # ç”Ÿæˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨±å¯/ä¸è¨±å¯ã®åˆ‡æ›¿ï¼ˆç›¸äº’æ’ä»–ï¼‰
    gen_group = parser.add_mutually_exclusive_group()
    gen_group.add_argument(
        '--allow-generated',
        action='store_true',
        help='ã€éæ¨å¥¨ã€‘ç”Ÿæˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¨±å¯ï¼ˆç¾åœ¨ã¯ç„¡åŠ¹åŒ–æ¸ˆã¿ï¼‰'
    )
    gen_group.add_argument(
        '--disallow-generated',
        action='store_true',
        help='ç”Ÿæˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æ˜ç¤ºçš„ã«ç¦æ­¢ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰'
    )

    # X APIä½¿ç”¨å¯å¦ã®åˆ‡æ›¿ï¼ˆç›¸äº’æ’ä»–ï¼‰
    x_api_group = parser.add_mutually_exclusive_group()
    x_api_group.add_argument(
        '--use-x-api',
        action='store_true',
        help='X APIã‚’ä½¿ç”¨ã™ã‚‹ï¼ˆæ˜ç¤ºçš„ã«æœ‰åŠ¹åŒ–ï¼‰'
    )
    x_api_group.add_argument(
        '--no-x-api',
        action='store_true',
        help='X APIã‚’ä½¿ç”¨ã—ãªã„ï¼ˆç„¡åŠ¹åŒ–ï¼‰'
    )

    args = parser.parse_args()

    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š
    logging.getLogger().setLevel(getattr(logging, args.log_level))

    # secrets.toml ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
    logger.info(f"ğŸ“– è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {args.secrets}")
    secrets = load_secrets_from_toml(args.secrets)
    if not secrets:
        logger.error("âŒ secrets.toml ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    # MODE ã«é–¢ã‚ã‚‰ãšç”Ÿæˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯å¸¸ã«ç„¡åŠ¹åŒ–
    mode_val = (secrets.get('MODE') or os.environ.get('MODE') or 'dev').lower()
    global ALLOW_GENERATED_FLAG
    ALLOW_GENERATED_FLAG = False
    if args.allow_generated:
        logger.warning("âš ï¸ --allow-generated ã¯ç„¡åŠ¹ã§ã™ã€‚å®Ÿãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    elif args.disallow_generated:
        logger.info("--disallow-generated æŒ‡å®š: ç”Ÿæˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ç„¡åŠ¹ (æ—¢å®šå€¤)")
    else:
        logger.info("ç”Ÿæˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨±å¯: Falseï¼ˆå›ºå®šé‹ç”¨ï¼‰")

    # X APIä½¿ç”¨å¯å¦ã‚’æ±ºå®š
    if args.use_x_api:
        use_x_api = True
        logger.info("X APIä½¿ç”¨: True (--use-x-apiæŒ‡å®š)")
    elif args.no_x_api:
        use_x_api = False
        logger.info("X APIä½¿ç”¨: False (--no-x-apiæŒ‡å®š)")
    else:
        # æœªæŒ‡å®šã®å ´åˆã¯å¾“æ¥ã©ãŠã‚ŠX_BEARER_TOKENã®å­˜åœ¨ã§åˆ¤å®š
        bearer_token = os.environ.get("X_BEARER_TOKEN")
        use_x_api = bool(bearer_token and bearer_token != "your_x_bearer_token_here")
        logger.info(f"X APIä½¿ç”¨: {use_x_api} (X_BEARER_TOKEN{'è¨­å®šæ¸ˆã¿' if use_x_api else 'æœªè¨­å®š'})")

    # API åˆæœŸåŒ–
    logger.info("ğŸ”§ APIåˆæœŸåŒ–ä¸­...")
    grok_api = load_grok_api_from_env()
    if not grok_api:
        logger.error("âŒ Grok API ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚GROK_API_KEY ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    # =============================================================================
    # Stage 2.5: Discover ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚¢ã‚«ã‚¦ãƒ³ãƒˆç™ºè¦‹ï¼‰
    # =============================================================================
    if args.discover_keyword or args.discover_random or args.preset or args.diversity_sampling:
        logger.info("=" * 80)
        logger.info("ğŸ” Stage 2.5: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç™ºè¦‹ãƒ¢ãƒ¼ãƒ‰")
        logger.info("=" * 80)

        # å¼•æ•°ã®æ¤œè¨¼ï¼ˆåŒæ™‚ã«è¤‡æ•°ã®ç™ºè¦‹ãƒ¢ãƒ¼ãƒ‰ã‚’æŒ‡å®šã—ãªã„ï¼‰
        mode_count = sum([
            bool(args.discover_keyword),
            bool(args.discover_random),
            bool(args.preset),
            bool(args.diversity_sampling)
        ])
        if mode_count > 1:
            logger.error("âŒ --discover-keyword, --discover-random, --preset, --diversity-sampling ã¯åŒæ™‚ã«1ã¤ã ã‘æŒ‡å®šã—ã¦ãã ã•ã„")
            sys.exit(1)

        # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç™ºè¦‹å®Ÿè¡Œ
        if use_x_api:
            x_api = load_x_api_from_env(use_x_api=True)
        else:
            x_api = None
            logger.info("X APIã‚’ç„¡åŠ¹åŒ–ã—ã¦ã„ã¾ã™ï¼ˆ--no-x-apiæŒ‡å®šã¾ãŸã¯X_BEARER_TOKENæœªè¨­å®šï¼‰")
        
        saved_path = discover_and_save_accounts(
            grok_api=grok_api,
            keyword=args.discover_keyword,
            random=args.discover_random,
            max_results=min(args.max_results, 100),  # ä¸Šé™100
            dry_run=args.dry_run,
            category=args.category,
            preset=args.preset,
            x_api=x_api,
            diversity_sampling=args.diversity_sampling,
            sampling_method=args.sampling_method,
            prefer_x_api=args.prefer_x_api,
            fallback_to_grok=not args.no_fallback_grok
        )

        if saved_path:
            logger.info("âœ… ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç™ºè¦‹ãŒå®Œäº†ã—ã¾ã—ãŸ")
            sys.exit(0)
        else:
            logger.error("âŒ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç™ºè¦‹ã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)

    # =============================================================================
    # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚¢ã‚«ã‚¦ãƒ³ãƒˆä¸€æ‹¬å‡¦ç†ï¼‰
    # =============================================================================
    if not args.accounts_file:
        logger.error("âŒ accounts_file ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚--discover-keyword ã¾ãŸã¯ --discover-random ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        parser.print_help()
        sys.exit(1)

    if use_x_api:
        x_api = load_x_api_from_env(use_x_api=True)
        if x_api:
            logger.info("âœ… X API v2 ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
        else:
            logger.info("â„¹ï¸  X API v2 ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ (Grok Web Search ã‚’ä½¿ç”¨)")
    else:
        x_api = None
        logger.info("X APIã‚’ç„¡åŠ¹åŒ–ã—ã¦ã„ã¾ã™ï¼ˆ--no-x-apiæŒ‡å®šã¾ãŸã¯X_BEARER_TOKENæœªè¨­å®šï¼‰")

    # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿ï¼ˆStage 2.5: discovery source ã®çµ±è¨ˆã‚’å–å¾—ï¼‰
    logger.info(f"ğŸ“‹ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿: {args.accounts_file}")

    # ã¾ãš metadata ä»˜ãã§èª­ã¿è¾¼ã‚“ã§ discovery source ã‚’ç¢ºèª
    accounts_with_meta = read_accounts_from_file(args.accounts_file, with_metadata=True)
    accounts = [acc['handle'] if isinstance(acc, dict) else acc for acc in accounts_with_meta]

    if not accounts:
        logger.error("âŒ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    logger.info(f"âœ… {len(accounts)} ä»¶ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # Discovery source ã®çµ±è¨ˆã‚’è¡¨ç¤ºï¼ˆStage 2.5ï¼‰
    if isinstance(accounts_with_meta[0], dict):
        from collections import Counter
        source_counts = Counter([acc['source'] for acc in accounts_with_meta])
        if any(s in source_counts for s in ['grok_keyword', 'grok_random']):
            logger.info("ğŸ“Š ç™ºè¦‹å…ƒå†…è¨³:")
            for source, count in sorted(source_counts.items()):
                if source in ['grok_keyword', 'grok_random']:
                    logger.info(f"  ğŸ” {source}: {count}")
                elif source == 'unknown':
                    logger.info(f"  â“ {source}: {count}")

    # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
    results = process_accounts_batch(
        accounts=accounts,
        grok_api=grok_api,
        x_api=x_api,
        batch_size=args.batch_size,
        enable_web_enrichment=not args.no_web_enrichment,
        force_refresh=args.force_refresh
    )

    # ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ç‡ã®é–¾å€¤ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5%ï¼‰
    GENERATED_RATIO_THRESHOLD = 5.0  # 5%è¶…éã§ã‚¨ãƒ©ãƒ¼
    generated_ratio = results.get('generated_data_ratio', 0.0)
    
    if generated_ratio > GENERATED_RATIO_THRESHOLD:
        logger.error("")
        logger.error("=" * 80)
        logger.error(f"âŒ ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ç‡ãŒé–¾å€¤ã‚’è¶…é: {generated_ratio:.1f}% > {GENERATED_RATIO_THRESHOLD}%")
        logger.error("=" * 80)
        logger.error("é‹ç”¨ãƒ¢ãƒ¼ãƒ‰ã§ã¯ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã¯è¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        logger.error("å®Ÿãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’å†è©¦è¡Œã™ã‚‹ã‹ã€--disallow-generated ãƒ•ãƒ©ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        logger.error("=" * 80)
        sys.exit(1)
    
    # çµ‚äº†ã‚³ãƒ¼ãƒ‰
    if results['failed'] > 0:
        logger.warning(f"âš ï¸  {results['failed']} ä»¶ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§å‡¦ç†ãŒå¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
    else:
        logger.info("âœ… ã™ã¹ã¦ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        sys.exit(0)


if __name__ == "__main__":
    main()
